{
  "papers": [
    {
      "title": "Knowledge Graph semantic enhancement of input data for improving AI",
      "abstract": "Intelligent systems designed using machine learning algorithms require a\nlarge number of labeled data. Background knowledge provides complementary, real\nworld factual information that can augment the limited labeled data to train a\nmachine learning algorithm. The term Knowledge Graph (KG) is in vogue as for\nmany practical applications, it is convenient and useful to organize this\nbackground knowledge in the form of a graph. Recent academic research and\nimplemented industrial intelligent systems have shown promising performance for\nmachine learning algorithms that combine training data with a knowledge graph.\nIn this article, we discuss the use of relevant KGs to enhance input data for\ntwo applications that use machine learning -- recommendation and community\ndetection. The KG improves both accuracy and explainability",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Intelligent systems designed using machine learning algorithms require a\nlarge number of labeled data. Background knowledge provides complementary, real\nworld factual information that can augment the limited labeled data to train a\nmachine learning algorithm. The term Knowledge Graph (KG) is in vogue as for\nmany practical applications, it is convenient and useful to organize this\nbackground knowledge in the form of a graph. Recent academic research and\nimplemented industrial intelligent systems have shown promising performance for\nmachine learning algorithms that combine training data with a knowledge graph.\nIn this article, we discuss the use of relevant KGs to enhance input data for\ntwo applications that use machine learning -- recommendation and community\ndetection. The KG improves both accuracy and explainability",
      "word_count": 121,
      "core_id": 85661335,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990069"
    },
    {
      "title": "Machine Learning For Planetary Mining Applications",
      "abstract": "Robotic mining could prove to be an efficient method of mining resources for extended missions on the Moon or Mars. One component of robotic mining is scouting an area for resources to be mined by other robotic systems. Writing controllers for scouting can be difficult due to the need for fault tolerance, inter-agent cooperation, and agent problem solving. Reinforcement learning could solve these problems by enabling the scouts to learn to improve their performance over time. This work is divided into two sections, with each section addressing the use of machine learning in this domain. The first contribution of this work focuses on the application of reinforcement learning to mining mission analysis. Various mission parameters were modified and control policies were learned. Then agent performance was used to assess the effect of the mission parameters on the performance of the mission. The second contribution of this work explores the potential use of reinforcement learning to learn a controller for the scouts. Through learning, these scouts would improve their ability to map their surroundings over time",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "Robotic mining could prove to be an efficient method of mining resources for extended missions on the Moon or Mars. One component of robotic mining is scouting an area for resources to be mined by other robotic systems. Writing controllers for scouting can be difficult due to the need for fault tolerance, inter-agent cooperation, and agent problem solving. Reinforcement learning could solve these problems by enabling the scouts to learn to improve their performance over time. This work is divided into two sections, with each section addressing the use of machine learning in this domain. The first contribution of this work focuses on the application of reinforcement learning to mining mission analysis. Various mission parameters were modified and control policies were learned. Then agent performance was used to assess the effect of the mission parameters on the performance of the mission. The second contribution of this work explores the potential use of reinforcement learning to learn a controller for the scouts. Through learning, these scouts would improve their ability to map their surroundings over time",
      "word_count": 175,
      "core_id": 78479320,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990113"
    },
    {
      "title": "Automatic differentiation in machine learning: a survey",
      "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\nmachine learning. Automatic differentiation (AD), also called algorithmic\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\nmore general than backpropagation for efficiently and accurately evaluating\nderivatives of numeric functions expressed as computer programs. AD is a small\nbut established field with applications in areas including computational fluid\ndynamics, atmospheric sciences, and engineering design optimization. Until very\nrecently, the fields of machine learning and AD have largely been unaware of\neach other and, in some cases, have independently discovered each other's\nresults. Despite its relevance, general-purpose AD has been missing from the\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\nunder the names \"dynamic computational graphs\" and \"differentiable\nprogramming\". We survey the intersection of AD and machine learning, cover\napplications where AD has direct relevance, and address the main implementation\ntechniques. By precisely defining the main differentiation techniques and their\ninterrelationships, we aim to bring clarity to the usage of the terms\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\nthese are encountered more and more in machine learning settings.Comment: 43 pages, 5 figure",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\nmachine learning. Automatic differentiation (AD), also called algorithmic\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\nmore general than backpropagation for efficiently and accurately evaluating\nderivatives of numeric functions expressed as computer programs. AD is a small\nbut established field with applications in areas including computational fluid\ndynamics, atmospheric sciences, and engineering design optimization. Until very\nrecently, the fields of machine learning and AD have largely been unaware of\neach other and, in some cases, have independently discovered each other's\nresults. Despite its relevance, general-purpose AD has been missing from the\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\nunder the names \"dynamic computational graphs\" and \"differentiable\nprogramming\". We survey the intersection of AD and machine learning, cover\napplications where AD has direct relevance, and address the main implementation\ntechniques. By precisely defining the main differentiation techniques and their\ninterrelationships, we aim to bring clarity to the usage of the terms\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\nthese are encountered more and more in machine learning settings.Comment: 43 pages, 5 figure",
      "word_count": 191,
      "core_id": 18046535,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990163"
    },
    {
      "title": "Dynamic Control Flow in Large-Scale Machine Learning",
      "abstract": "Many recent machine learning models rely on fine-grained dynamic control flow\nfor training and inference. In particular, models based on recurrent neural\nnetworks and on reinforcement learning depend on recurrence relations,\ndata-dependent conditional execution, and other features that call for dynamic\ncontrol flow. These applications benefit from the ability to make rapid\ncontrol-flow decisions across a set of computing devices in a distributed\nsystem. For performance, scalability, and expressiveness, a machine learning\nsystem must support dynamic control flow in distributed and heterogeneous\nenvironments.\n  This paper presents a programming model for distributed machine learning that\nsupports dynamic control flow. We describe the design of the programming model,\nand its implementation in TensorFlow, a distributed machine learning system.\nOur approach extends the use of dataflow graphs to represent machine learning\nmodels, offering several distinctive features. First, the branches of\nconditionals and bodies of loops can be partitioned across many machines to run\non a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.\nSecond, programs written in our model support automatic differentiation and\ndistributed gradient computations, which are necessary for training machine\nlearning models that use control flow. Third, our choice of non-strict\nsemantics enables multiple loop iterations to execute in parallel across\nmachines, and to overlap compute and I/O operations.\n  We have done our work in the context of TensorFlow, and it has been used\nextensively in research and production. We evaluate it using several real-world\napplications, and demonstrate its performance and scalability.Comment: Appeared in EuroSys 2018. 14 pages, 16 figure",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Many recent machine learning models rely on fine-grained dynamic control flow\nfor training and inference. In particular, models based on recurrent neural\nnetworks and on reinforcement learning depend on recurrence relations,\ndata-dependent conditional execution, and other features that call for dynamic\ncontrol flow. These applications benefit from the ability to make rapid\ncontrol-flow decisions across a set of computing devices in a distributed\nsystem. For performance, scalability, and expressiveness, a machine learning\nsystem must support dynamic control flow in distributed and heterogeneous\nenvironments.\n  This paper presents a programming model for distributed machine learning that\nsupports dynamic control flow. We describe the design of the programming model,\nand its implementation in TensorFlow, a distributed machine learning system.\nOur approach extends the use of dataflow graphs to represent machine learning\nmodels, offering several distinctive features. First, the branches of\nconditionals and bodies of loops can be partitioned across many machines to run\non a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.\nSecond, programs written in our model support automatic differentiation and\ndistributed gradient computations, which are necessary for training machine\nlearning models that use control flow. Third, our choice of non-strict\nsemantics enables multiple loop iterations to execute in parallel across\nmachines, and to overlap compute and I/O operations.\n  We have done our work in the context of TensorFlow, and it has been used\nextensively in research and production. We evaluate it using several real-world\napplications, and demonstrate its performance and scalability.Comment: Appeared in EuroSys 2018. 14 pages, 16 figure",
      "word_count": 251,
      "core_id": 51392219,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990212"
    },
    {
      "title": "A Supervised STDP-based Training Algorithm for Living Neural Networks",
      "abstract": "Neural networks have shown great potential in many applications like speech\nrecognition, drug discovery, image classification, and object detection. Neural\nnetwork models are inspired by biological neural networks, but they are\noptimized to perform machine learning tasks on digital computers. The proposed\nwork explores the possibilities of using living neural networks in vitro as\nbasic computational elements for machine learning applications. A new\nsupervised STDP-based learning algorithm is proposed in this work, which\nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the\nMNIST benchmark for handwritten digit recognition.Comment: 5 pages, 3 figures, Accepted by ICASSP 201",
      "keywords": [],
      "paper_type": "empirical",
      "field": "computer_science",
      "content": "Neural networks have shown great potential in many applications like speech\nrecognition, drug discovery, image classification, and object detection. Neural\nnetwork models are inspired by biological neural networks, but they are\noptimized to perform machine learning tasks on digital computers. The proposed\nwork explores the possibilities of using living neural networks in vitro as\nbasic computational elements for machine learning applications. A new\nsupervised STDP-based learning algorithm is proposed in this work, which\nconsiders neuron engineering constrains. A 74.7% accuracy is achieved on the\nMNIST benchmark for handwritten digit recognition.Comment: 5 pages, 3 figures, Accepted by ICASSP 201",
      "word_count": 98,
      "core_id": 45152197,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990244"
    },
    {
      "title": "Automatic generation of hardware Tree Classifiers",
      "abstract": "Machine Learning is growing in popularity and spreading across different fields for various applications. Due to this trend, machine learning algorithms use different hardware platforms and are being experimented to obtain high test accuracy and throughput. FPGAs are well-suited hardware platform for machine learning because of its re-programmability and lower power consumption. Programming using FPGAs for machine learning algorithms requires substantial engineering time and effort compared to software implementation. We propose a software assisted design flow to program FPGA for machine learning algorithms using our hardware library. The hardware library is highly parameterized and it accommodates Tree Classifiers. As of now, our library consists of the components required to implement decision trees and random forests. The whole automation is wrapped around using a python script which takes you from the first step of having a dataset and design choices to the last step of having a hardware descriptive code for the trained machine learning model",
      "keywords": [],
      "paper_type": "comparative",
      "field": "computer_science",
      "content": "Machine Learning is growing in popularity and spreading across different fields for various applications. Due to this trend, machine learning algorithms use different hardware platforms and are being experimented to obtain high test accuracy and throughput. FPGAs are well-suited hardware platform for machine learning because of its re-programmability and lower power consumption. Programming using FPGAs for machine learning algorithms requires substantial engineering time and effort compared to software implementation. We propose a software assisted design flow to program FPGA for machine learning algorithms using our hardware library. The hardware library is highly parameterized and it accommodates Tree Classifiers. As of now, our library consists of the components required to implement decision trees and random forests. The whole automation is wrapped around using a python script which takes you from the first step of having a dataset and design choices to the last step of having a hardware descriptive code for the trained machine learning model",
      "word_count": 155,
      "core_id": 46231635,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990270"
    },
    {
      "title": "A complexity analysis of statistical learning algorithms",
      "abstract": "We apply information-based complexity analysis to support vector machine\n(SVM) algorithms, with the goal of a comprehensive continuous algorithmic\nanalysis of such algorithms. This involves complexity measures in which some\nhigher order operations (e.g., certain optimizations) are considered primitive\nfor the purposes of measuring complexity. We consider classes of information\noperators and algorithms made up of scaled families, and investigate the\nutility of scaling the complexities to minimize error. We look at the division\nof statistical learning into information and algorithmic components, at the\ncomplexities of each, and at applications to support vector machine (SVM) and\nmore general machine learning algorithms. We give applications to SVM\nalgorithms graded into linear and higher order components, and give an example\nin biomedical informatics",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "We apply information-based complexity analysis to support vector machine\n(SVM) algorithms, with the goal of a comprehensive continuous algorithmic\nanalysis of such algorithms. This involves complexity measures in which some\nhigher order operations (e.g., certain optimizations) are considered primitive\nfor the purposes of measuring complexity. We consider classes of information\noperators and algorithms made up of scaled families, and investigate the\nutility of scaling the complexities to minimize error. We look at the division\nof statistical learning into information and algorithmic components, at the\ncomplexities of each, and at applications to support vector machine (SVM) and\nmore general machine learning algorithms. We give applications to SVM\nalgorithms graded into linear and higher order components, and give an example\nin biomedical informatics",
      "word_count": 121,
      "core_id": 17030072,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990288"
    },
    {
      "title": "Adaptive Sequential Optimization with Applications to Machine Learning",
      "abstract": "A framework is introduced for solving a sequence of slowly changing\noptimization problems, including those arising in regression and classification\napplications, using optimization algorithms such as stochastic gradient descent\n(SGD). The optimization problems change slowly in the sense that the minimizers\nchange at either a fixed or bounded rate. A method based on estimates of the\nchange in the minimizers and properties of the optimization algorithm is\nintroduced for adaptively selecting the number of samples needed from the\ndistributions underlying each problem in order to ensure that the excess risk,\ni.e., the expected gap between the loss achieved by the approximate minimizer\nproduced by the optimization algorithm and the exact minimizer, does not exceed\na target level. Experiments with synthetic and real data are used to confirm\nthat this approach performs well.Comment: submitted to ICASSP 2016, extended versio",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "A framework is introduced for solving a sequence of slowly changing\noptimization problems, including those arising in regression and classification\napplications, using optimization algorithms such as stochastic gradient descent\n(SGD). The optimization problems change slowly in the sense that the minimizers\nchange at either a fixed or bounded rate. A method based on estimates of the\nchange in the minimizers and properties of the optimization algorithm is\nintroduced for adaptively selecting the number of samples needed from the\ndistributions underlying each problem in order to ensure that the excess risk,\ni.e., the expected gap between the loss achieved by the approximate minimizer\nproduced by the optimization algorithm and the exact minimizer, does not exceed\na target level. Experiments with synthetic and real data are used to confirm\nthat this approach performs well.Comment: submitted to ICASSP 2016, extended versio",
      "word_count": 138,
      "core_id": 24732979,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990308"
    },
    {
      "title": "Conditionals in Homomorphic Encryption and Machine Learning Applications",
      "abstract": "Homomorphic encryption aims at allowing computations on encrypted data\nwithout decryption other than that of the final result. This could provide an\nelegant solution to the issue of privacy preservation in data-based\napplications, such as those using machine learning, but several open issues\nhamper this plan. In this work we assess the possibility for homomorphic\nencryption to fully implement its program without relying on other techniques,\nsuch as multiparty computation (SMPC), which may be impossible in many use\ncases (for instance due to the high level of communication required). We\nproceed in two steps: i) on the basis of the structured program theorem\n(Bohm-Jacopini theorem) we identify the relevant minimal set of operations\nhomomorphic encryption must be able to perform to implement any algorithm; and\nii) we analyse the possibility to solve -- and propose an implementation for --\nthe most fundamentally relevant issue as it emerges from our analysis, that is,\nthe implementation of conditionals (requiring comparison and selection/jump\noperations). We show how this issue clashes with the fundamental requirements\nof homomorphic encryption and could represent a drawback for its use as a\ncomplete solution for privacy preservation in data-based applications, in\nparticular machine learning ones. Our approach for comparisons is novel and\nentirely embedded in homomorphic encryption, while previous studies relied on\nother techniques, such as SMPC, demanding high level of communication among\nparties, and decryption of intermediate results from data-owners. Our protocol\nis also provably safe (sharing the same safety as the homomorphic encryption\nschemes), differently from other techniques such as\nOrder-Preserving/Revealing-Encryption (OPE/ORE).Comment: 14 pages, 1 figure, corrected typos, added introductory pedagogical\n  section on polynomial approximatio",
      "keywords": [],
      "paper_type": "comparative",
      "field": "computer_science",
      "content": "Homomorphic encryption aims at allowing computations on encrypted data\nwithout decryption other than that of the final result. This could provide an\nelegant solution to the issue of privacy preservation in data-based\napplications, such as those using machine learning, but several open issues\nhamper this plan. In this work we assess the possibility for homomorphic\nencryption to fully implement its program without relying on other techniques,\nsuch as multiparty computation (SMPC), which may be impossible in many use\ncases (for instance due to the high level of communication required). We\nproceed in two steps: i) on the basis of the structured program theorem\n(Bohm-Jacopini theorem) we identify the relevant minimal set of operations\nhomomorphic encryption must be able to perform to implement any algorithm; and\nii) we analyse the possibility to solve -- and propose an implementation for --\nthe most fundamentally relevant issue as it emerges from our analysis, that is,\nthe implementation of conditionals (requiring comparison and selection/jump\noperations). We show how this issue clashes with the fundamental requirements\nof homomorphic encryption and could represent a drawback for its use as a\ncomplete solution for privacy preservation in data-based applications, in\nparticular machine learning ones. Our approach for comparisons is novel and\nentirely embedded in homomorphic encryption, while previous studies relied on\nother techniques, such as SMPC, demanding high level of communication among\nparties, and decryption of intermediate results from data-owners. Our protocol\nis also provably safe (sharing the same safety as the homomorphic encryption\nschemes), differently from other techniques such as\nOrder-Preserving/Revealing-Encryption (OPE/ORE).Comment: 14 pages, 1 figure, corrected typos, added introductory pedagogical\n  section on polynomial approximatio",
      "word_count": 269,
      "core_id": 54166104,
      "year": null,
      "processed_date": "2025-07-01T10:45:29.990353"
    },
    {
      "title": "Deep Learning for Forecasting Stock Returns in the Cross-Section",
      "abstract": "Many studies have been undertaken by using machine learning techniques,\nincluding neural networks, to predict stock returns. Recently, a method known\nas deep learning, which achieves high performance mainly in image recognition\nand speech recognition, has attracted attention in the machine learning field.\nThis paper implements deep learning to predict one-month-ahead stock returns in\nthe cross-section in the Japanese stock market and investigates the performance\nof the method. Our results show that deep neural networks generally outperform\nshallow neural networks, and the best networks also outperform representative\nmachine learning models. These results indicate that deep learning shows\npromise as a skillful machine learning method to predict stock returns in the\ncross-section.Comment: 12 pages, 2 figures, 8 tables, accepted at PAKDD 201",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Many studies have been undertaken by using machine learning techniques,\nincluding neural networks, to predict stock returns. Recently, a method known\nas deep learning, which achieves high performance mainly in image recognition\nand speech recognition, has attracted attention in the machine learning field.\nThis paper implements deep learning to predict one-month-ahead stock returns in\nthe cross-section in the Japanese stock market and investigates the performance\nof the method. Our results show that deep neural networks generally outperform\nshallow neural networks, and the best networks also outperform representative\nmachine learning models. These results indicate that deep learning shows\npromise as a skillful machine learning method to predict stock returns in the\ncross-section.Comment: 12 pages, 2 figures, 8 tables, accepted at PAKDD 201",
      "word_count": 121,
      "core_id": 47140066,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523656"
    },
    {
      "title": "Efficient Deep Feature Learning and Extraction via StochasticNets",
      "abstract": "Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.Comment: 10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1508.0546",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.Comment: 10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1508.0546",
      "word_count": 271,
      "core_id": 24752999,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523730"
    },
    {
      "title": "Short-Term Load Forecasting of Natural Gas with Deep Neural Network Regression",
      "abstract": "Deep neural networks are proposed for short-term natural gas load forecasting. Deep learning has proven to be a powerful tool for many classification problems seeing significant use in machine learning fields such as image recognition and speech processing. We provide an overview of natural gas forecasting. Next, the deep learning method, contrastive divergence is explained. We compare our proposed deep neural network method to a linear regression model and a traditional artificial neural network on 62 operating areas, each of which has at least 10 years of data. The proposed deep network outperforms traditional artificial neural networks by 9.83% weighted mean absolute percent error (WMAPE)",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Deep neural networks are proposed for short-term natural gas load forecasting. Deep learning has proven to be a powerful tool for many classification problems seeing significant use in machine learning fields such as image recognition and speech processing. We provide an overview of natural gas forecasting. Next, the deep learning method, contrastive divergence is explained. We compare our proposed deep neural network method to a linear regression model and a traditional artificial neural network on 62 operating areas, each of which has at least 10 years of data. The proposed deep network outperforms traditional artificial neural networks by 9.83% weighted mean absolute percent error (WMAPE)",
      "word_count": 105,
      "core_id": 41660677,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523762"
    },
    {
      "title": "Neurogenesis Deep Learning",
      "abstract": "Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms.Comment: 8 pages, 8 figures, Accepted to 2017 International Joint Conference\n  on Neural Networks (IJCNN 2017",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms.Comment: 8 pages, 8 figures, Accepted to 2017 International Joint Conference\n  on Neural Networks (IJCNN 2017",
      "word_count": 201,
      "core_id": 37687589,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523801"
    },
    {
      "title": "Every Local Minimum Value is the Global Minimum Value of Induced Model\n  in Non-convex Machine Learning",
      "abstract": "For nonconvex optimization in machine learning, this article proves that\nevery local minimum achieves the globally optimal value of the perturbable\ngradient basis model at any differentiable point. As a result, nonconvex\nmachine learning is theoretically as supported as convex machine learning with\na handcrafted basis in terms of the loss at differentiable local minima, except\nin the case when a preference is given to the handcrafted basis over the\nperturbable gradient basis. The proofs of these results are derived under mild\nassumptions. Accordingly, the proven results are directly applicable to many\nmachine learning models, including practical deep neural networks, without any\nmodification of practical methods. Furthermore, as special cases of our general\nresults, this article improves or complements several state-of-the-art\ntheoretical results on deep neural networks, deep residual networks, and\noverparameterized deep neural networks with a unified proof technique and novel\ngeometric insights. A special case of our results also contributes to the\ntheoretical foundation of representation learning.Comment: Neural computation, MIT pres",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "For nonconvex optimization in machine learning, this article proves that\nevery local minimum achieves the globally optimal value of the perturbable\ngradient basis model at any differentiable point. As a result, nonconvex\nmachine learning is theoretically as supported as convex machine learning with\na handcrafted basis in terms of the loss at differentiable local minima, except\nin the case when a preference is given to the handcrafted basis over the\nperturbable gradient basis. The proofs of these results are derived under mild\nassumptions. Accordingly, the proven results are directly applicable to many\nmachine learning models, including practical deep neural networks, without any\nmodification of practical methods. Furthermore, as special cases of our general\nresults, this article improves or complements several state-of-the-art\ntheoretical results on deep neural networks, deep residual networks, and\noverparameterized deep neural networks with a unified proof technique and novel\ngeometric insights. A special case of our results also contributes to the\ntheoretical foundation of representation learning.Comment: Neural computation, MIT pres",
      "word_count": 163,
      "core_id": 58965578,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523826"
    },
    {
      "title": "Verifying Properties of Binarized Deep Neural Networks",
      "abstract": "Understanding properties of deep neural networks is an important challenge in\ndeep learning. In this paper, we take a step in this direction by proposing a\nrigorous way of verifying properties of a popular class of neural networks,\nBinarized Neural Networks, using the well-developed means of Boolean\nsatisfiability. Our main contribution is a construction that creates a\nrepresentation of a binarized neural network as a Boolean formula. Our encoding\nis the first exact Boolean representation of a deep neural network. Using this\nencoding, we leverage the power of modern SAT solvers along with a proposed\ncounterexample-guided search procedure to verify various properties of these\nnetworks. A particular focus will be on the critical property of robustness to\nadversarial perturbations. For this property, our experimental results\ndemonstrate that our approach scales to medium-size deep neural networks used\nin image classification tasks. To the best of our knowledge, this is the first\nwork on verifying properties of deep neural networks using an exact Boolean\nencoding of the network.Comment: 10 page",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Understanding properties of deep neural networks is an important challenge in\ndeep learning. In this paper, we take a step in this direction by proposing a\nrigorous way of verifying properties of a popular class of neural networks,\nBinarized Neural Networks, using the well-developed means of Boolean\nsatisfiability. Our main contribution is a construction that creates a\nrepresentation of a binarized neural network as a Boolean formula. Our encoding\nis the first exact Boolean representation of a deep neural network. Using this\nencoding, we leverage the power of modern SAT solvers along with a proposed\ncounterexample-guided search procedure to verify various properties of these\nnetworks. A particular focus will be on the critical property of robustness to\nadversarial perturbations. For this property, our experimental results\ndemonstrate that our approach scales to medium-size deep neural networks used\nin image classification tasks. To the best of our knowledge, this is the first\nwork on verifying properties of deep neural networks using an exact Boolean\nencoding of the network.Comment: 10 page",
      "word_count": 168,
      "core_id": 44608929,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523859"
    },
    {
      "title": "Deep Learning in the Automotive Industry: Applications and Tools",
      "abstract": "Deep Learning refers to a set of machine learning techniques that utilize\nneural networks with many hidden layers for tasks, such as image\nclassification, speech recognition, language understanding. Deep learning has\nbeen proven to be very effective in these domains and is pervasively used by\nmany Internet services. In this paper, we describe different automotive uses\ncases for deep learning in particular in the domain of computer vision. We\nsurveys the current state-of-the-art in libraries, tools and infrastructures\n(e.\\,g.\\ GPUs and clouds) for implementing, training and deploying deep neural\nnetworks. We particularly focus on convolutional neural networks and computer\nvision use cases, such as the visual inspection process in manufacturing plants\nand the analysis of social media data. To train neural networks, curated and\nlabeled datasets are essential. In particular, both the availability and scope\nof such datasets is typically very limited. A main contribution of this paper\nis the creation of an automotive dataset, that allows us to learn and\nautomatically recognize different vehicle properties. We describe an end-to-end\ndeep learning application utilizing a mobile app for data collection and\nprocess support, and an Amazon-based cloud backend for storage and training.\nFor training we evaluate the use of cloud and on-premises infrastructures\n(including multiple GPUs) in conjunction with different neural network\narchitectures and frameworks. We assess both the training times as well as the\naccuracy of the classifier. Finally, we demonstrate the effectiveness of the\ntrained classifier in a real world setting during manufacturing process.Comment: 10 page",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "social_sciences",
      "content": "Deep Learning refers to a set of machine learning techniques that utilize\nneural networks with many hidden layers for tasks, such as image\nclassification, speech recognition, language understanding. Deep learning has\nbeen proven to be very effective in these domains and is pervasively used by\nmany Internet services. In this paper, we describe different automotive uses\ncases for deep learning in particular in the domain of computer vision. We\nsurveys the current state-of-the-art in libraries, tools and infrastructures\n(e.\\,g.\\ GPUs and clouds) for implementing, training and deploying deep neural\nnetworks. We particularly focus on convolutional neural networks and computer\nvision use cases, such as the visual inspection process in manufacturing plants\nand the analysis of social media data. To train neural networks, curated and\nlabeled datasets are essential. In particular, both the availability and scope\nof such datasets is typically very limited. A main contribution of this paper\nis the creation of an automotive dataset, that allows us to learn and\nautomatically recognize different vehicle properties. We describe an end-to-end\ndeep learning application utilizing a mobile app for data collection and\nprocess support, and an Amazon-based cloud backend for storage and training.\nFor training we evaluate the use of cloud and on-premises infrastructures\n(including multiple GPUs) in conjunction with different neural network\narchitectures and frameworks. We assess both the training times as well as the\naccuracy of the classifier. Finally, we demonstrate the effectiveness of the\ntrained classifier in a real world setting during manufacturing process.Comment: 10 page",
      "word_count": 248,
      "core_id": 42861659,
      "year": null,
      "processed_date": "2025-07-01T10:45:33.523885"
    },
    {
      "title": "Intelligent systems in manufacturing: current developments and future prospects",
      "abstract": "Global competition and rapidly changing customer requirements are demanding increasing changes in manufacturing environments. Enterprises are required to constantly redesign their products and continuously reconfigure their manufacturing systems. Traditional approaches to manufacturing systems do not fully satisfy this new situation. Many authors have proposed that artificial intelligence will bring the flexibility and efficiency needed by manufacturing systems. This paper is a review of artificial intelligence techniques used in manufacturing systems. The paper first defines the components of a simplified intelligent manufacturing systems (IMS), the different Artificial Intelligence (AI) techniques to be considered and then shows how these AI techniques are used for the components of IMS",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "Global competition and rapidly changing customer requirements are demanding increasing changes in manufacturing environments. Enterprises are required to constantly redesign their products and continuously reconfigure their manufacturing systems. Traditional approaches to manufacturing systems do not fully satisfy this new situation. Many authors have proposed that artificial intelligence will bring the flexibility and efficiency needed by manufacturing systems. This paper is a review of artificial intelligence techniques used in manufacturing systems. The paper first defines the components of a simplified intelligent manufacturing systems (IMS), the different Artificial Intelligence (AI) techniques to be considered and then shows how these AI techniques are used for the components of IMS",
      "word_count": 106,
      "core_id": 8372517,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.928779"
    },
    {
      "title": "Artificial Intelligence in the Context of Human Consciousness",
      "abstract": "Artificial intelligence (AI) can be defined as the ability of a machine to learn and make decisions based on acquired information. AI’s development has incited rampant public speculation regarding the singularity theory: a futuristic phase in which intelligent machines are capable of creating increasingly intelligent systems. Its implications, combined with the close relationship between humanity and their machines, make achieving understanding both natural and artificial intelligence imperative. Researchers are continuing to discover natural processes responsible for essential human skills like decision-making, understanding language, and performing multiple processes simultaneously. Artificial intelligence attempts to simulate these functions through techniques like artificial neural networks, Markov Decision Processes, Human Language Technology, and Multi-Agent Systems, which rely upon a combination of mathematical models and hardware",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "Artificial intelligence (AI) can be defined as the ability of a machine to learn and make decisions based on acquired information. AI’s development has incited rampant public speculation regarding the singularity theory: a futuristic phase in which intelligent machines are capable of creating increasingly intelligent systems. Its implications, combined with the close relationship between humanity and their machines, make achieving understanding both natural and artificial intelligence imperative. Researchers are continuing to discover natural processes responsible for essential human skills like decision-making, understanding language, and performing multiple processes simultaneously. Artificial intelligence attempts to simulate these functions through techniques like artificial neural networks, Markov Decision Processes, Human Language Technology, and Multi-Agent Systems, which rely upon a combination of mathematical models and hardware",
      "word_count": 120,
      "core_id": 44761588,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.928830"
    },
    {
      "title": "A Neural-CBR System for Real Property Valuation",
      "abstract": "In recent times, the application of artificial intelligence (AI) techniques for real property valuation has been on the\n\nincrease. Some expert systems that leveraged on machine intelligence concepts include rule-based reasoning, case-based\n\nreasoning and artificial neural networks. These approaches have proved reliable thus far and in certain cases outperformed\n\nthe use of statistical predictive models such as hedonic regression, logistic regression, and discriminant analysis. However,\n\nindividual artificial intelligence approaches have their inherent limitations. These limitations hamper the quality of\n\ndecision support they proffer when used alone for real property valuation. In this paper, we present a Neural-CBR system\n\nfor real property valuation, which is based on a hybrid architecture that combines Artificial Neural Networks and Case-\n\nBased Reasoning techniques. An evaluation of the system was conducted and the experimental results revealed that the\n\nsystem has higher satisfactory level of performance when compared with individual Artificial Neural Network and Case-\n\nBased Reasoning systems",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "In recent times, the application of artificial intelligence (AI) techniques for real property valuation has been on the\n\nincrease. Some expert systems that leveraged on machine intelligence concepts include rule-based reasoning, case-based\n\nreasoning and artificial neural networks. These approaches have proved reliable thus far and in certain cases outperformed\n\nthe use of statistical predictive models such as hedonic regression, logistic regression, and discriminant analysis. However,\n\nindividual artificial intelligence approaches have their inherent limitations. These limitations hamper the quality of\n\ndecision support they proffer when used alone for real property valuation. In this paper, we present a Neural-CBR system\n\nfor real property valuation, which is based on a hybrid architecture that combines Artificial Neural Networks and Case-\n\nBased Reasoning techniques. An evaluation of the system was conducted and the experimental results revealed that the\n\nsystem has higher satisfactory level of performance when compared with individual Artificial Neural Network and Case-\n\nBased Reasoning systems",
      "word_count": 152,
      "core_id": 33137077,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.928866"
    },
    {
      "title": "Do Chatbots Dream of Androids? Prospects for the Technological Development of Artificial Intelligence and Robotics",
      "abstract": "The article discusses the main trends in the development of artificial intelligence systems and robotics (AI&R). The main question that is considered in this context is whether artificial systems are going to become more and more anthropomorphic, both intellectually and physically. In the current article, the author analyzes the current state and prospects of technological development of artificial intelligence and robotics, and also determines the main aspects of the impact of these technologies on society and economy, indicating the geopolitical strategic nature of this influence. The author considers various approaches to the definition of artificial intelligence and robotics, focusing on the subject-oriented and functional ones. It also compares AI&R abilities and human abilities in areas such as categorization, pattern recognition, planning and decision making, etc. Based on this comparison, we investigate in which areas AI&R’s performance is inferior to a human, and in which cases it is superior to one. The modern achievements in the field of robotics and artificial intelligence create the necessary basis for further discussion of the applicability of goal setting in engineering, in the form of a Turing test. It is shown that development of AI&R is associated with certain contradictions that impede the application of Turing’s methodology in its usual format. The basic contradictions in the development of AI&R technologies imply that there is to be a transition to a post-Turing methodology for assessing engineering implementations of artificial intelligence and robotics. In such implementations, on the one hand, the ‘Turing wall’ is removed, and on the other hand, artificial intelligence gets its physical implementation",
      "keywords": [],
      "paper_type": "comparative",
      "field": "computer_science",
      "content": "The article discusses the main trends in the development of artificial intelligence systems and robotics (AI&R). The main question that is considered in this context is whether artificial systems are going to become more and more anthropomorphic, both intellectually and physically. In the current article, the author analyzes the current state and prospects of technological development of artificial intelligence and robotics, and also determines the main aspects of the impact of these technologies on society and economy, indicating the geopolitical strategic nature of this influence. The author considers various approaches to the definition of artificial intelligence and robotics, focusing on the subject-oriented and functional ones. It also compares AI&R abilities and human abilities in areas such as categorization, pattern recognition, planning and decision making, etc. Based on this comparison, we investigate in which areas AI&R’s performance is inferior to a human, and in which cases it is superior to one. The modern achievements in the field of robotics and artificial intelligence create the necessary basis for further discussion of the applicability of goal setting in engineering, in the form of a Turing test. It is shown that development of AI&R is associated with certain contradictions that impede the application of Turing’s methodology in its usual format. The basic contradictions in the development of AI&R technologies imply that there is to be a transition to a post-Turing methodology for assessing engineering implementations of artificial intelligence and robotics. In such implementations, on the one hand, the ‘Turing wall’ is removed, and on the other hand, artificial intelligence gets its physical implementation",
      "word_count": 259,
      "core_id": 8859381,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.928939"
    },
    {
      "title": "Towards an Intelligent Database System Founded on the SP Theory of\n  Computing and Cognition",
      "abstract": "The SP theory of computing and cognition, described in previous publications,\nis an attractive model for intelligent databases because it provides a simple\nbut versatile format for different kinds of knowledge, it has capabilities in\nartificial intelligence, and it can also function like established database\nmodels when that is required.\n  This paper describes how the SP model can emulate other models used in\ndatabase applications and compares the SP model with those other models. The\nartificial intelligence capabilities of the SP model are reviewed and its\nrelationship with other artificial intelligence systems is described. Also\nconsidered are ways in which current prototypes may be translated into an\n'industrial strength' working system",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "The SP theory of computing and cognition, described in previous publications,\nis an attractive model for intelligent databases because it provides a simple\nbut versatile format for different kinds of knowledge, it has capabilities in\nartificial intelligence, and it can also function like established database\nmodels when that is required.\n  This paper describes how the SP model can emulate other models used in\ndatabase applications and compares the SP model with those other models. The\nartificial intelligence capabilities of the SP model are reviewed and its\nrelationship with other artificial intelligence systems is described. Also\nconsidered are ways in which current prototypes may be translated into an\n'industrial strength' working system",
      "word_count": 111,
      "core_id": 944251,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.928955"
    },
    {
      "title": "Knowledge representation by connection matrices: A method for the on-board implementation of large expert systems",
      "abstract": "Extremely large knowledge sources and efficient knowledge access characterizing future real-life artificial intelligence applications represent crucial requirements for on-board artificial intelligence systems due to obvious computer time and storage constraints on spacecraft. A type of knowledge representation and corresponding reasoning mechanism is proposed which is particularly suited for the efficient processing of such large knowledge bases in expert systems",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Extremely large knowledge sources and efficient knowledge access characterizing future real-life artificial intelligence applications represent crucial requirements for on-board artificial intelligence systems due to obvious computer time and storage constraints on spacecraft. A type of knowledge representation and corresponding reasoning mechanism is proposed which is particularly suited for the efficient processing of such large knowledge bases in expert systems",
      "word_count": 59,
      "core_id": 24881820,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.928984"
    },
    {
      "title": "Faith in the Algorithm, Part 1: Beyond the Turing Test",
      "abstract": "Since the Turing test was first proposed by Alan Turing in 1950, the primary\ngoal of artificial intelligence has been predicated on the ability for\ncomputers to imitate human behavior. However, the majority of uses for the\ncomputer can be said to fall outside the domain of human abilities and it is\nexactly outside of this domain where computers have demonstrated their greatest\ncontribution to intelligence. Another goal for artificial intelligence is one\nthat is not predicated on human mimicry, but instead, on human amplification.\nThis article surveys various systems that contribute to the advancement of\nhuman and social intelligence",
      "keywords": [],
      "paper_type": "technical",
      "field": "social_sciences",
      "content": "Since the Turing test was first proposed by Alan Turing in 1950, the primary\ngoal of artificial intelligence has been predicated on the ability for\ncomputers to imitate human behavior. However, the majority of uses for the\ncomputer can be said to fall outside the domain of human abilities and it is\nexactly outside of this domain where computers have demonstrated their greatest\ncontribution to intelligence. Another goal for artificial intelligence is one\nthat is not predicated on human mimicry, but instead, on human amplification.\nThis article surveys various systems that contribute to the advancement of\nhuman and social intelligence",
      "word_count": 100,
      "core_id": 635570,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929010"
    },
    {
      "title": "Computational aerodynamics and artificial intelligence",
      "abstract": "The general principles of artificial intelligence are reviewed and speculations are made concerning how knowledge based systems can accelerate the process of acquiring new knowledge in aerodynamics, how computational fluid dynamics may use expert systems, and how expert systems may speed the design and development process. In addition, the anatomy of an idealized expert system called AERODYNAMICIST is discussed. Resource requirements for using artificial intelligence in computational fluid dynamics and aerodynamics are examined. Three main conclusions are presented. First, there are two related aspects of computational aerodynamics: reasoning and calculating. Second, a substantial portion of reasoning can be achieved with artificial intelligence. It offers the opportunity of using computers as reasoning machines to set the stage for efficient calculating. Third, expert systems are likely to be new assets of institutions involved in aeronautics for various tasks of computational aerodynamics",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "The general principles of artificial intelligence are reviewed and speculations are made concerning how knowledge based systems can accelerate the process of acquiring new knowledge in aerodynamics, how computational fluid dynamics may use expert systems, and how expert systems may speed the design and development process. In addition, the anatomy of an idealized expert system called AERODYNAMICIST is discussed. Resource requirements for using artificial intelligence in computational fluid dynamics and aerodynamics are examined. Three main conclusions are presented. First, there are two related aspects of computational aerodynamics: reasoning and calculating. Second, a substantial portion of reasoning can be achieved with artificial intelligence. It offers the opportunity of using computers as reasoning machines to set the stage for efficient calculating. Third, expert systems are likely to be new assets of institutions involved in aeronautics for various tasks of computational aerodynamics",
      "word_count": 139,
      "core_id": 24893761,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929027"
    },
    {
      "title": "SIR: A New Wireless Sensor Network Routing Protocol Based on Artificial Intelligence",
      "abstract": "Currently, Wireless Sensor Networks (WSNs) are formed by\r\nhundreds of low energy and low cost micro-electro-mechanical systems.\r\nRouting and low power consumption have become important research issues\r\nto interconnect this kind of networks. However, conventional Quality\r\nof Service routing models, are not suitable for ad hoc sensor networks,\r\ndue to the dynamic nature of such systems. This paper introduces a new\r\nQoS-driven routing algorithm, named SIR: Sensor Intelligence Routing.\r\nWe have designed an artificial neural network based on Kohonen self\r\norganizing features map. Every node implements this artificial neural\r\nnetwork forming a distributed intelligence and ubiquitous computing\r\nsystem",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Currently, Wireless Sensor Networks (WSNs) are formed by\r\nhundreds of low energy and low cost micro-electro-mechanical systems.\r\nRouting and low power consumption have become important research issues\r\nto interconnect this kind of networks. However, conventional Quality\r\nof Service routing models, are not suitable for ad hoc sensor networks,\r\ndue to the dynamic nature of such systems. This paper introduces a new\r\nQoS-driven routing algorithm, named SIR: Sensor Intelligence Routing.\r\nWe have designed an artificial neural network based on Kohonen self\r\norganizing features map. Every node implements this artificial neural\r\nnetwork forming a distributed intelligence and ubiquitous computing\r\nsystem",
      "word_count": 98,
      "core_id": 52449946,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929057"
    },
    {
      "title": "Artificial in its own right",
      "abstract": "Artificial Cells, , Artificial Ecologies, Artificial Intelligence, Bio-Inspired Hardware Systems,  Computational Autopoiesis, Computational Biology, Computational Embryology, Computational Evolution, Morphogenesis, Cyborgization, Digital Evolution, Evolvable Hardware, Cyborgs, Mathematical Biology, Nanotechnology, Posthuman, Transhuman",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Artificial Cells, , Artificial Ecologies, Artificial Intelligence, Bio-Inspired Hardware Systems,  Computational Autopoiesis, Computational Biology, Computational Embryology, Computational Evolution, Morphogenesis, Cyborgization, Digital Evolution, Evolvable Hardware, Cyborgs, Mathematical Biology, Nanotechnology, Posthuman, Transhuman",
      "word_count": 30,
      "core_id": 8853365,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929074"
    },
    {
      "title": "Financial Computational Intelligence",
      "abstract": "Artificial intelligence decision support system is always a popular topic in providing the human with an optimized decision recommendation when operating under uncertainty in complex environments. The particular focus of our discussion is to compare different methods of artificial intelligence decision support systems in the investment domain – the goal of investment decision-making is to select an optimal portfolio that satisfies the investor’s objective, or, in other words, to maximize the investment returns under the constraints given by investors. In this study we apply several artificial intelligence systems like Influence Diagram (a special type of Bayesian network), Decision Tree and Neural Network to get experimental comparison analysis to help users to intelligently select the best portfoliArtificial intelligence, neural network, decision tree, bayesian network",
      "keywords": [],
      "paper_type": "comparative",
      "field": "computer_science",
      "content": "Artificial intelligence decision support system is always a popular topic in providing the human with an optimized decision recommendation when operating under uncertainty in complex environments. The particular focus of our discussion is to compare different methods of artificial intelligence decision support systems in the investment domain – the goal of investment decision-making is to select an optimal portfolio that satisfies the investor’s objective, or, in other words, to maximize the investment returns under the constraints given by investors. In this study we apply several artificial intelligence systems like Influence Diagram (a special type of Bayesian network), Decision Tree and Neural Network to get experimental comparison analysis to help users to intelligently select the best portfoliArtificial intelligence, neural network, decision tree, bayesian network",
      "word_count": 123,
      "core_id": 2892785,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929109"
    },
    {
      "title": "A review of the field of artificial intelligence and its possible applications to nasa objectives  final report",
      "abstract": "Artificial intelligence - control, data gathering, and data analyzing systems desig",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "Artificial intelligence - control, data gathering, and data analyzing systems desig",
      "word_count": 11,
      "core_id": 43630529,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929115"
    },
    {
      "title": "Chord progressions selection based on song audio features",
      "abstract": "HAIS: International Conference on Hybrid Artificial Intelligence Systems (13th, 2018, Oviedo)This research has been funded by the Spanish MINECO project TIN2017-87600-P",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "HAIS: International Conference on Hybrid Artificial Intelligence Systems (13th, 2018, Oviedo)This research has been funded by the Spanish MINECO project TIN2017-87600-P",
      "word_count": 21,
      "core_id": 276396833,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929129"
    },
    {
      "title": "Experimental set-up for investigation of fault diagnosis of a centrifugal pump",
      "abstract": "Centrifugal pumps are complex machines which can experience different types of fault. Condition monitoring can be used in centrifugal pump fault detection through vibration analysis for mechanical and hydraulic forces. Vibration analysis methods have the potential to be combined with artificial intelligence systems where an automatic diagnostic method can be approached. An automatic fault diagnosis approach could be a good option to minimize human error and to provide a precise machine fault classification. This work aims to introduce an approach to centrifugal pump fault diagnosis based on artificial intelligence and genetic algorithm systems. An overview of the future works, research methodology and proposed experimental setup is presented and discussed. The expected results and outcomes based on the experimental work are illustrated",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "Centrifugal pumps are complex machines which can experience different types of fault. Condition monitoring can be used in centrifugal pump fault detection through vibration analysis for mechanical and hydraulic forces. Vibration analysis methods have the potential to be combined with artificial intelligence systems where an automatic diagnostic method can be approached. An automatic fault diagnosis approach could be a good option to minimize human error and to provide a precise machine fault classification. This work aims to introduce an approach to centrifugal pump fault diagnosis based on artificial intelligence and genetic algorithm systems. An overview of the future works, research methodology and proposed experimental setup is presented and discussed. The expected results and outcomes based on the experimental work are illustrated",
      "word_count": 121,
      "core_id": 8753818,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929156"
    },
    {
      "title": "Fighter aircraft flight control technology design requirements",
      "abstract": "The evolution of fighter aircraft flight control technology is briefly surveyed. Systems engineering, battle damage considerations for adaptive flutter suppression, in-flight simulation, and artificial intelligence are briefly discussed",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "The evolution of fighter aircraft flight control technology is briefly surveyed. Systems engineering, battle damage considerations for adaptive flutter suppression, in-flight simulation, and artificial intelligence are briefly discussed",
      "word_count": 28,
      "core_id": 4474168,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929170"
    },
    {
      "title": "Reducing offline evaluation bias of collaborative filtering algorithms",
      "abstract": "Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper presents a new application\nof a weighted offline evaluation to reduce this bias for collaborative\nfiltering algorithms.Comment: European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.137-142, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper presents a new application\nof a weighted offline evaluation to reduce this bias for collaborative\nfiltering algorithms.Comment: European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.137-142, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015",
      "word_count": 106,
      "core_id": 18070255,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929201"
    },
    {
      "title": "Artificial intelligence and space power systems automation",
      "abstract": "Various applications of artificial intelligence to space electrical power systems are discussed. An overview is given of completed, on-going, and planned knowledge-based system activities. These applications include the Nickel-Cadmium Battery Expert System (NICBES) (the expert system interfaced with the Hubble Space Telescope electrical power system test bed); the early work with the Space Station Experiment Scheduler (SSES); the three expert systems under development in the space station advanced development effort in the core module power management and distribution system test bed; planned cooperation of expert systems in the Core Module Power Management and Distribution (CM/PMAD) system breadboard with expert systems for the space station at other research centers; and the intelligent data reduction expert system under development",
      "keywords": [],
      "paper_type": "technical",
      "field": "business",
      "content": "Various applications of artificial intelligence to space electrical power systems are discussed. An overview is given of completed, on-going, and planned knowledge-based system activities. These applications include the Nickel-Cadmium Battery Expert System (NICBES) (the expert system interfaced with the Hubble Space Telescope electrical power system test bed); the early work with the Space Station Experiment Scheduler (SSES); the three expert systems under development in the space station advanced development effort in the core module power management and distribution system test bed; planned cooperation of expert systems in the Core Module Power Management and Distribution (CM/PMAD) system breadboard with expert systems for the space station at other research centers; and the intelligent data reduction expert system under development",
      "word_count": 117,
      "core_id": 24886705,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929235"
    },
    {
      "title": "Algorithms and Complexity Results for Persuasive Argumentation",
      "abstract": "The study of arguments as abstract entities and their interaction as\nintroduced by Dung (Artificial Intelligence 177, 1995) has become one of the\nmost active research branches within Artificial Intelligence and Reasoning. A\nmain issue for abstract argumentation systems is the selection of acceptable\nsets of arguments. Value-based argumentation, as introduced by Bench-Capon (J.\nLogic Comput. 13, 2003), extends Dung's framework. It takes into account the\nrelative strength of arguments with respect to some ranking representing an\naudience: an argument is subjectively accepted if it is accepted with respect\nto some audience, it is objectively accepted if it is accepted with respect to\nall audiences. Deciding whether an argument is subjectively or objectively\naccepted, respectively, are computationally intractable problems. In fact, the\nproblems remain intractable under structural restrictions that render the main\ncomputational problems for non-value-based argumentation systems tractable. In\nthis paper we identify nontrivial classes of value-based argumentation systems\nfor which the acceptance problems are polynomial-time tractable. The classes\nare defined by means of structural restrictions in terms of the underlying\ngraphical structure of the value-based system. Furthermore we show that the\nacceptance problems are intractable for two classes of value-based systems that\nwhere conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007)",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "The study of arguments as abstract entities and their interaction as\nintroduced by Dung (Artificial Intelligence 177, 1995) has become one of the\nmost active research branches within Artificial Intelligence and Reasoning. A\nmain issue for abstract argumentation systems is the selection of acceptable\nsets of arguments. Value-based argumentation, as introduced by Bench-Capon (J.\nLogic Comput. 13, 2003), extends Dung's framework. It takes into account the\nrelative strength of arguments with respect to some ranking representing an\naudience: an argument is subjectively accepted if it is accepted with respect\nto some audience, it is objectively accepted if it is accepted with respect to\nall audiences. Deciding whether an argument is subjectively or objectively\naccepted, respectively, are computationally intractable problems. In fact, the\nproblems remain intractable under structural restrictions that render the main\ncomputational problems for non-value-based argumentation systems tractable. In\nthis paper we identify nontrivial classes of value-based argumentation systems\nfor which the acceptance problems are polynomial-time tractable. The classes\nare defined by means of structural restrictions in terms of the underlying\ngraphical structure of the value-based system. Furthermore we show that the\nacceptance problems are intractable for two classes of value-based systems that\nwhere conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007)",
      "word_count": 205,
      "core_id": 753529,
      "year": null,
      "processed_date": "2025-07-01T10:45:35.929264"
    },
    {
      "title": "ABDN at SemEval-2018 Task 10 : recognising discriminative attributes using context embeddings and WordNet",
      "abstract": "This paper describes the system that we submitted for SemEval-2018 task 10: capturing discriminative attributes. Our system is built upon a simple idea of measuring the attribute word’s similarity with each of the two semantically similar words, based on an extended word embedding method and WordNet. Instead of computing the similarities between the attribute and semantically similar words by using standard word embeddings, we propose a novel method that combines word and context embeddings which can better measure similarities. Our model is simple and effective, which achieves an average F1 score of 0.62 on the test set",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "This paper describes the system that we submitted for SemEval-2018 task 10: capturing discriminative attributes. Our system is built upon a simple idea of measuring the attribute word’s similarity with each of the two semantically similar words, based on an extended word embedding method and WordNet. Instead of computing the similarities between the attribute and semantically similar words by using standard word embeddings, we propose a novel method that combines word and context embeddings which can better measure similarities. Our model is simple and effective, which achieves an average F1 score of 0.62 on the test set",
      "word_count": 97,
      "core_id": 8456703,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435032"
    },
    {
      "title": "Readers and Reading in the First World War",
      "abstract": "This essay consists of three individually authored and interlinked sections. In ‘A Digital Humanities Approach’, Francesca Benatti looks at datasets and databases (including the UK Reading Experience Database) and shows how a systematic, macro-analytical use of digital humanities tools and resources might yield answers to some key questions about reading in the First World War. In ‘Reading behind the Wire in the First World War’ Edmund G. C. King scrutinizes the reading practices and preferences of Allied prisoners of war in Mainz, showing that reading circumscribed by the contingencies of a prison camp created an unique literary community, whose legacy can be traced through their literary output after the war. In ‘Book-hunger in Salonika’, Shafquat Towheed examines the record of a single reader in a specific and fairly static frontline, and argues that in the case of the Salonika campaign, reading communities emerged in close proximity to existing centres of print culture. The focus of this essay moves from the general to the particular, from the scoping of large datasets, to the analyses of identified readers within a specific geographical and temporal space. The authors engage with the wider issues and problems of recovering, interpreting, visualizing, narrating, and representing readers in the First World War",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "This essay consists of three individually authored and interlinked sections. In ‘A Digital Humanities Approach’, Francesca Benatti looks at datasets and databases (including the UK Reading Experience Database) and shows how a systematic, macro-analytical use of digital humanities tools and resources might yield answers to some key questions about reading in the First World War. In ‘Reading behind the Wire in the First World War’ Edmund G. C. King scrutinizes the reading practices and preferences of Allied prisoners of war in Mainz, showing that reading circumscribed by the contingencies of a prison camp created an unique literary community, whose legacy can be traced through their literary output after the war. In ‘Book-hunger in Salonika’, Shafquat Towheed examines the record of a single reader in a specific and fairly static frontline, and argues that in the case of the Salonika campaign, reading communities emerged in close proximity to existing centres of print culture. The focus of this essay moves from the general to the particular, from the scoping of large datasets, to the analyses of identified readers within a specific geographical and temporal space. The authors engage with the wider issues and problems of recovering, interpreting, visualizing, narrating, and representing readers in the First World War",
      "word_count": 205,
      "core_id": 8059692,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435109"
    },
    {
      "title": "How we do things with words: Analyzing text as social and cultural data",
      "abstract": "In this article we describe our experiences with computational text analysis.\nWe hope to achieve three primary goals. First, we aim to shed light on thorny\nissues not always at the forefront of discussions about computational text\nanalysis methods. Second, we hope to provide a set of best practices for\nworking with thick social and cultural concepts. Our guidance is based on our\nown experiences and is therefore inherently imperfect. Still, given our\ndiversity of disciplinary backgrounds and research practices, we hope to\ncapture a range of ideas and identify commonalities that will resonate for\nmany. And this leads to our final goal: to help promote interdisciplinary\ncollaborations. Interdisciplinary insights and partnerships are essential for\nrealizing the full potential of any computational text analysis that involves\nsocial and cultural concepts, and the more we are able to bridge these divides,\nthe more fruitful we believe our work will be",
      "keywords": [],
      "paper_type": "analytical",
      "field": "social_sciences",
      "content": "In this article we describe our experiences with computational text analysis.\nWe hope to achieve three primary goals. First, we aim to shed light on thorny\nissues not always at the forefront of discussions about computational text\nanalysis methods. Second, we hope to provide a set of best practices for\nworking with thick social and cultural concepts. Our guidance is based on our\nown experiences and is therefore inherently imperfect. Still, given our\ndiversity of disciplinary backgrounds and research practices, we hope to\ncapture a range of ideas and identify commonalities that will resonate for\nmany. And this leads to our final goal: to help promote interdisciplinary\ncollaborations. Interdisciplinary insights and partnerships are essential for\nrealizing the full potential of any computational text analysis that involves\nsocial and cultural concepts, and the more we are able to bridge these divides,\nthe more fruitful we believe our work will be",
      "word_count": 149,
      "core_id": 89007840,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435141"
    },
    {
      "title": "Natural language processing",
      "abstract": "Beginning with the basic issues of NLP, this chapter aims to chart the major research activities in this area since the last ARIST Chapter in 1996 (Haas, 1996), including: (i) natural language text processing systems - text summarization, information extraction, information retrieval, etc., including domain-specific applications; (ii) natural language interfaces; (iii) NLP in the context of www and digital libraries ; and (iv) evaluation of NLP systems",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Beginning with the basic issues of NLP, this chapter aims to chart the major research activities in this area since the last ARIST Chapter in 1996 (Haas, 1996), including: (i) natural language text processing systems - text summarization, information extraction, information retrieval, etc., including domain-specific applications; (ii) natural language interfaces; (iii) NLP in the context of www and digital libraries ; and (iv) evaluation of NLP systems",
      "word_count": 67,
      "core_id": 4189744,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435164"
    },
    {
      "title": "Text content and task performance in the evaluation of a natural language generation system",
      "abstract": "An important question in the evaluation of Natural Language Generation systems concerns the relationship between textual characteristics and task performance. If the results of task-based evaluation can be correlated to properties of the text, there are better prospects for improving the system. The present paper investigates this relationship by focusing on the outcomes of a task-based evaluation of a system that generates summaries of patient data, attempting to correlate these with the results of an analysis of the system’s texts, compared to a set of gold standard human-authored summaries.peer-reviewe",
      "keywords": [],
      "paper_type": "review",
      "field": "medicine",
      "content": "An important question in the evaluation of Natural Language Generation systems concerns the relationship between textual characteristics and task performance. If the results of task-based evaluation can be correlated to properties of the text, there are better prospects for improving the system. The present paper investigates this relationship by focusing on the outcomes of a task-based evaluation of a system that generates summaries of patient data, attempting to correlate these with the results of an analysis of the system’s texts, compared to a set of gold standard human-authored summaries.peer-reviewe",
      "word_count": 89,
      "core_id": 45741943,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435182"
    },
    {
      "title": "Robust Processing of Natural Language",
      "abstract": "Previous approaches to robustness in natural language processing usually\ntreat deviant input by relaxing grammatical constraints whenever a successful\nanalysis cannot be provided by ``normal'' means. This schema implies, that\nerror detection always comes prior to error handling, a behaviour which hardly\ncan compete with its human model, where many erroneous situations are treated\nwithout even noticing them.\n  The paper analyses the necessary preconditions for achieving a higher degree\nof robustness in natural language processing and suggests a quite different\napproach based on a procedure for structural disambiguation. It not only offers\nthe possibility to cope with robustness issues in a more natural way but\neventually might be suited to accommodate quite different aspects of robust\nbehaviour within a single framework.Comment: 16 pages, LaTeX, uses pstricks.sty, pstricks.tex, pstricks.pro,\n  pst-node.sty, pst-node.tex, pst-node.pro. To appear in: Proc. KI-95, 19th\n  German Conference on Artificial Intelligence, Bielefeld (Germany), Lecture\n  Notes in Computer Science, Springer 199",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "Previous approaches to robustness in natural language processing usually\ntreat deviant input by relaxing grammatical constraints whenever a successful\nanalysis cannot be provided by ``normal'' means. This schema implies, that\nerror detection always comes prior to error handling, a behaviour which hardly\ncan compete with its human model, where many erroneous situations are treated\nwithout even noticing them.\n  The paper analyses the necessary preconditions for achieving a higher degree\nof robustness in natural language processing and suggests a quite different\napproach based on a procedure for structural disambiguation. It not only offers\nthe possibility to cope with robustness issues in a more natural way but\neventually might be suited to accommodate quite different aspects of robust\nbehaviour within a single framework.Comment: 16 pages, LaTeX, uses pstricks.sty, pstricks.tex, pstricks.pro,\n  pst-node.sty, pst-node.tex, pst-node.pro. To appear in: Proc. KI-95, 19th\n  German Conference on Artificial Intelligence, Bielefeld (Germany), Lecture\n  Notes in Computer Science, Springer 199",
      "word_count": 151,
      "core_id": 875165,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435206"
    },
    {
      "title": "Software Infrastructure for Natural Language Processing",
      "abstract": "We classify and review current approaches to software infrastructure for\nresearch, development and delivery of NLP systems. The task is motivated by a\ndiscussion of current trends in the field of NLP and Language Engineering. We\ndescribe a system called GATE (a General Architecture for Text Engineering)\nthat provides a software infrastructure on top of which heterogeneous NLP\nprocessing modules may be evaluated and refined individually, or may be\ncombined into larger application systems. GATE aims to support both researchers\nand developers working on component technologies (e.g. parsing, tagging,\nmorphological analysis) and those working on developing end-user applications\n(e.g. information extraction, text summarisation, document generation, machine\ntranslation, and second language learning). GATE promotes reuse of component\ntechnology, permits specialisation and collaboration in large-scale projects,\nand allows for the comparison and evaluation of alternative technologies. The\nfirst release of GATE is now available - see\nhttp://www.dcs.shef.ac.uk/research/groups/nlp/gate/Comment: LaTeX, uses aclap.sty, 8 page",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "We classify and review current approaches to software infrastructure for\nresearch, development and delivery of NLP systems. The task is motivated by a\ndiscussion of current trends in the field of NLP and Language Engineering. We\ndescribe a system called GATE (a General Architecture for Text Engineering)\nthat provides a software infrastructure on top of which heterogeneous NLP\nprocessing modules may be evaluated and refined individually, or may be\ncombined into larger application systems. GATE aims to support both researchers\nand developers working on component technologies (e.g. parsing, tagging,\nmorphological analysis) and those working on developing end-user applications\n(e.g. information extraction, text summarisation, document generation, machine\ntranslation, and second language learning). GATE promotes reuse of component\ntechnology, permits specialisation and collaboration in large-scale projects,\nand allows for the comparison and evaluation of alternative technologies. The\nfirst release of GATE is now available - see\nhttp://www.dcs.shef.ac.uk/research/groups/nlp/gate/Comment: LaTeX, uses aclap.sty, 8 page",
      "word_count": 150,
      "core_id": 875325,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435225"
    },
    {
      "title": "Generating Media Background Checks for Automated Source Critical Reasoning",
      "abstract": "Not everything on the internet is true. This unfortunate fact requires both humans and models to perform complex reasoning about credibility when working with retrieved information. In NLP, this problem has seen little attention. Indeed, retrieval-augmented models are not typically expected to distrust retrieved documents. Human experts overcome the challenge by gathering signals about the context, reliability, and tendency of source documents - that is, they perform source criticism. We propose a novel NLP task focused on finding and summarising such signals. We introduce a new dataset of 6,709 \"media background checks\" derived from Media Bias / Fact Check, a volunteer-run website documenting media bias. We test open-source and closed-source LLM baselines with and without retrieval on this dataset, finding that retrieval greatly improves performance. We furthermore carry out human evaluation, demonstrating that 1) media background checks are helpful for humans, and 2) media background checks are helpful for retrieval-augmented models",
      "keywords": [],
      "paper_type": "empirical",
      "field": "computer_science",
      "content": "Not everything on the internet is true. This unfortunate fact requires both humans and models to perform complex reasoning about credibility when working with retrieved information. In NLP, this problem has seen little attention. Indeed, retrieval-augmented models are not typically expected to distrust retrieved documents. Human experts overcome the challenge by gathering signals about the context, reliability, and tendency of source documents - that is, they perform source criticism. We propose a novel NLP task focused on finding and summarising such signals. We introduce a new dataset of 6,709 \"media background checks\" derived from Media Bias / Fact Check, a volunteer-run website documenting media bias. We test open-source and closed-source LLM baselines with and without retrieval on this dataset, finding that retrieval greatly improves performance. We furthermore carry out human evaluation, demonstrating that 1) media background checks are helpful for humans, and 2) media background checks are helpful for retrieval-augmented models",
      "word_count": 151,
      "core_id": 170433578,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435274"
    },
    {
      "title": "Constraint Logic Programming for Natural Language Processing",
      "abstract": "This paper proposes an evaluation of the adequacy of the constraint logic\nprogramming paradigm for natural language processing. Theoretical aspects of\nthis question have been discussed in several works. We adopt here a pragmatic\npoint of view and our argumentation relies on concrete solutions. Using actual\ncontraints (in the CLP sense) is neither easy nor direct. However, CLP can\nimprove parsing techniques in several aspects such as concision, control,\nefficiency or direct representation of linguistic formalism. This discussion is\nillustrated by several examples and the presentation of an HPSG parser.Comment: 15 pages, uuencoded and compressed postscript to appear in\n  Proceedings of the 5th Int. Workshop on Natural Language Understanding and\n  Logic Programming. Lisbon, Portugal. 199",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "This paper proposes an evaluation of the adequacy of the constraint logic\nprogramming paradigm for natural language processing. Theoretical aspects of\nthis question have been discussed in several works. We adopt here a pragmatic\npoint of view and our argumentation relies on concrete solutions. Using actual\ncontraints (in the CLP sense) is neither easy nor direct. However, CLP can\nimprove parsing techniques in several aspects such as concision, control,\nefficiency or direct representation of linguistic formalism. This discussion is\nillustrated by several examples and the presentation of an HPSG parser.Comment: 15 pages, uuencoded and compressed postscript to appear in\n  Proceedings of the 5th Int. Workshop on Natural Language Understanding and\n  Logic Programming. Lisbon, Portugal. 199",
      "word_count": 115,
      "core_id": 875002,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435291"
    },
    {
      "title": "Evaluating algorithms for the generation of referring expressions : going beyond toy domains",
      "abstract": "We describe a corpus-based evaluation methodology, applied to a number of classic algorithms in the generation of referring expressions. Following up on earlier work involving very simple domains, this paper deals with the issues associated with domains that contain ‘real-life’ objects of some complexity. Results indicate that state of the art algorithms perform very differently when applied to a complex domain. Moreover, if a version of the Incremental Algorithm is used then it becomes of huge importance to select a good preference order. These results should contribute to a growing debate on the evaluation of nlg systems, arguing in favour of carefully constructed balanced and semantically transparent corpora.peer-reviewe",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "We describe a corpus-based evaluation methodology, applied to a number of classic algorithms in the generation of referring expressions. Following up on earlier work involving very simple domains, this paper deals with the issues associated with domains that contain ‘real-life’ objects of some complexity. Results indicate that state of the art algorithms perform very differently when applied to a complex domain. Moreover, if a version of the Incremental Algorithm is used then it becomes of huge importance to select a good preference order. These results should contribute to a growing debate on the evaluation of nlg systems, arguing in favour of carefully constructed balanced and semantically transparent corpora.peer-reviewe",
      "word_count": 108,
      "core_id": 44543844,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435315"
    },
    {
      "title": "Characterizing Phishing Threats with Natural Language Processing",
      "abstract": "Spear phishing is a widespread concern in the modern network security\nlandscape, but there are few metrics that measure the extent to which\nreconnaissance is performed on phishing targets. Spear phishing emails closely\nmatch the expectations of the recipient, based on details of their experiences\nand interests, making them a popular propagation vector for harmful malware. In\nthis work we use Natural Language Processing techniques to investigate a\nspecific real-world phishing campaign and quantify attributes that indicate a\ntargeted spear phishing attack. Our phishing campaign data sample comprises 596\nemails - all containing a web bug and a Curriculum Vitae (CV) PDF attachment -\nsent to our institution by a foreign IP space. The campaign was found to\nexclusively target specific demographics within our institution. Performing a\nsemantic similarity analysis between the senders' CV attachments and the\nrecipients' LinkedIn profiles, we conclude with high statistical certainty (p\n$< 10^{-4}$) that the attachments contain targeted rather than randomly\nselected material. Latent Semantic Analysis further demonstrates that\nindividuals who were a primary focus of the campaign received CVs that are\nhighly topically clustered. These findings differentiate this campaign from one\nthat leverages random spam.Comment: This paper has been accepted for publication by the IEEE Conference\n  on Communications and Network Security in September 2015 at Florence, Italy.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessibl",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "Spear phishing is a widespread concern in the modern network security\nlandscape, but there are few metrics that measure the extent to which\nreconnaissance is performed on phishing targets. Spear phishing emails closely\nmatch the expectations of the recipient, based on details of their experiences\nand interests, making them a popular propagation vector for harmful malware. In\nthis work we use Natural Language Processing techniques to investigate a\nspecific real-world phishing campaign and quantify attributes that indicate a\ntargeted spear phishing attack. Our phishing campaign data sample comprises 596\nemails - all containing a web bug and a Curriculum Vitae (CV) PDF attachment -\nsent to our institution by a foreign IP space. The campaign was found to\nexclusively target specific demographics within our institution. Performing a\nsemantic similarity analysis between the senders' CV attachments and the\nrecipients' LinkedIn profiles, we conclude with high statistical certainty (p\n$< 10^{-4}$) that the attachments contain targeted rather than randomly\nselected material. Latent Semantic Analysis further demonstrates that\nindividuals who were a primary focus of the campaign received CVs that are\nhighly topically clustered. These findings differentiate this campaign from one\nthat leverages random spam.Comment: This paper has been accepted for publication by the IEEE Conference\n  on Communications and Network Security in September 2015 at Florence, Italy.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessibl",
      "word_count": 229,
      "core_id": 18090422,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435357"
    },
    {
      "title": "Quantifying Uncertainties in Natural Language Processing Tasks",
      "abstract": "Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.Comment: To appear at AAAI 201",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.Comment: To appear at AAAI 201",
      "word_count": 100,
      "core_id": 54172332,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435379"
    },
    {
      "title": "Language Without Words: A Pointillist Model for Natural Language\n  Processing",
      "abstract": "This paper explores two separate questions: Can we perform natural language\nprocessing tasks without a lexicon?; and, Should we? Existing natural language\nprocessing techniques are either based on words as units or use units such as\ngrams only for basic classification tasks. How close can a machine come to\nreasoning about the meanings of words and phrases in a corpus without using any\nlexicon, based only on grams?\n  Our own motivation for posing this question is based on our efforts to find\npopular trends in words and phrases from online Chinese social media. This form\nof written Chinese uses so many neologisms, creative character placements, and\ncombinations of writing systems that it has been dubbed the \"Martian Language.\"\nReaders must often use visual queues, audible queues from reading out loud, and\ntheir knowledge and understanding of current events to understand a post. For\nanalysis of popular trends, the specific problem is that it is difficult to\nbuild a lexicon when the invention of new ways to refer to a word or concept is\neasy and common. For natural language processing in general, we argue in this\npaper that new uses of language in social media will challenge machines'\nabilities to operate with words as the basic unit of understanding, not only in\nChinese but potentially in other languages.Comment: 5 pages, 2 figure",
      "keywords": [],
      "paper_type": "analytical",
      "field": "social_sciences",
      "content": "This paper explores two separate questions: Can we perform natural language\nprocessing tasks without a lexicon?; and, Should we? Existing natural language\nprocessing techniques are either based on words as units or use units such as\ngrams only for basic classification tasks. How close can a machine come to\nreasoning about the meanings of words and phrases in a corpus without using any\nlexicon, based only on grams?\n  Our own motivation for posing this question is based on our efforts to find\npopular trends in words and phrases from online Chinese social media. This form\nof written Chinese uses so many neologisms, creative character placements, and\ncombinations of writing systems that it has been dubbed the \"Martian Language.\"\nReaders must often use visual queues, audible queues from reading out loud, and\ntheir knowledge and understanding of current events to understand a post. For\nanalysis of popular trends, the specific problem is that it is difficult to\nbuild a lexicon when the invention of new ways to refer to a word or concept is\neasy and common. For natural language processing in general, we argue in this\npaper that new uses of language in social media will challenge machines'\nabilities to operate with words as the basic unit of understanding, not only in\nChinese but potentially in other languages.Comment: 5 pages, 2 figure",
      "word_count": 222,
      "core_id": 17026386,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435408"
    },
    {
      "title": "SKOPE: A connectionist/symbolic architecture of spoken Korean processing",
      "abstract": "Spoken language processing requires speech and natural language integration.\nMoreover, spoken Korean calls for unique processing methodology due to its\nlinguistic characteristics. This paper presents SKOPE, a connectionist/symbolic\nspoken Korean processing engine, which emphasizes that: 1) connectionist and\nsymbolic techniques must be selectively applied according to their relative\nstrength and weakness, and 2) the linguistic characteristics of Korean must be\nfully considered for phoneme recognition, speech and language integration, and\nmorphological/syntactic processing. The design and implementation of SKOPE\ndemonstrates how connectionist/symbolic hybrid architectures can be constructed\nfor spoken agglutinative language processing. Also SKOPE presents many novel\nideas for speech and language processing. The phoneme recognition,\nmorphological analysis, and syntactic analysis experiments show that SKOPE is a\nviable approach for the spoken Korean processing.Comment: 8 pages, latex, use aaai.sty & aaai.bst, bibfile: nlpsp.bib, to be\n  presented at IJCAI95 workshops on new approaches to learning for natural\n  language processin",
      "keywords": [],
      "paper_type": "analytical",
      "field": "computer_science",
      "content": "Spoken language processing requires speech and natural language integration.\nMoreover, spoken Korean calls for unique processing methodology due to its\nlinguistic characteristics. This paper presents SKOPE, a connectionist/symbolic\nspoken Korean processing engine, which emphasizes that: 1) connectionist and\nsymbolic techniques must be selectively applied according to their relative\nstrength and weakness, and 2) the linguistic characteristics of Korean must be\nfully considered for phoneme recognition, speech and language integration, and\nmorphological/syntactic processing. The design and implementation of SKOPE\ndemonstrates how connectionist/symbolic hybrid architectures can be constructed\nfor spoken agglutinative language processing. Also SKOPE presents many novel\nideas for speech and language processing. The phoneme recognition,\nmorphological analysis, and syntactic analysis experiments show that SKOPE is a\nviable approach for the spoken Korean processing.Comment: 8 pages, latex, use aaai.sty & aaai.bst, bibfile: nlpsp.bib, to be\n  presented at IJCAI95 workshops on new approaches to learning for natural\n  language processin",
      "word_count": 147,
      "core_id": 875034,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435440"
    },
    {
      "title": "Virtual personal assistant",
      "abstract": "Abstract This report discusses ways in which new technology could be harnessed to create an intelligent Virtual Personal Assistant (VPA) with a focus on user-based information. It will look at examples of intelligent programs with natural language processing that are currently available, with different categories of support, and examine the potential usefulness of one specific piece of software as a VPA. This engages the ability to communicate socially through natural language processing, holding (and analysing) information within the context of the user. It is suggested that new technologies may soon make the idea of virtual personal assistants a reality. Experiments conducted on this system, combined with user testing, have provided evidence that a basic program with natural language processing algorithms in the form of a VPA, with basic natural language processing and the ability to function without the need for other type of human input (or programming) may already be viable",
      "keywords": [],
      "paper_type": "technical",
      "field": "social_sciences",
      "content": "Abstract This report discusses ways in which new technology could be harnessed to create an intelligent Virtual Personal Assistant (VPA) with a focus on user-based information. It will look at examples of intelligent programs with natural language processing that are currently available, with different categories of support, and examine the potential usefulness of one specific piece of software as a VPA. This engages the ability to communicate socially through natural language processing, holding (and analysing) information within the context of the user. It is suggested that new technologies may soon make the idea of virtual personal assistants a reality. Experiments conducted on this system, combined with user testing, have provided evidence that a basic program with natural language processing algorithms in the form of a VPA, with basic natural language processing and the ability to function without the need for other type of human input (or programming) may already be viable",
      "word_count": 151,
      "core_id": 18098189,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435470"
    },
    {
      "title": "Library Cataloguing and Role and Reference Grammar for Natural Language processing Applications",
      "abstract": "Several potential application of natural language processing have proven to be intractable. In this paper, we provide and overview of methods from library cataloguing and linguistics that have not yet been adopted by the natural language processing community and which could be used to help solve some of these problems",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Several potential application of natural language processing have proven to be intractable. In this paper, we provide and overview of methods from library cataloguing and linguistics that have not yet been adopted by the natural language processing community and which could be used to help solve some of these problems",
      "word_count": 50,
      "core_id": 17385399,
      "year": null,
      "processed_date": "2025-07-01T10:45:39.435485"
    },
    {
      "title": "Speech Processing in Computer Vision Applications",
      "abstract": "Deep learning has been recently proven to be a viable asset in determining features in the field of Speech Analysis. Deep learning methods like Convolutional Neural Networks facilitate the expansion of specific feature information in waveforms, allowing networks to create more feature dense representations of data. Our work attempts to address the problem of re-creating a face given a speaker\\u27s voice and speaker identification using deep learning methods. In this work, we first review the fundamental background in speech processing and its related applications. Then we introduce novel deep learning-based methods to speech feature analysis. Finally, we will present our deep learning approaches to speaker identification and speech to face synthesis. The presented method can convert a speaker audio sample to an image of their predicted face. This framework is composed of several chained together networks, each with an essential step in the conversion process. These include Audio embedding, encoding, and face generation networks, respectively. Our experiments show that certain features can map to the face and that with a speaker\\u27s voice, DNNs can create their face and that a GUI could be used in conjunction to display a speaker recognition network\\u27s data",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "Deep learning has been recently proven to be a viable asset in determining features in the field of Speech Analysis. Deep learning methods like Convolutional Neural Networks facilitate the expansion of specific feature information in waveforms, allowing networks to create more feature dense representations of data. Our work attempts to address the problem of re-creating a face given a speaker\\u27s voice and speaker identification using deep learning methods. In this work, we first review the fundamental background in speech processing and its related applications. Then we introduce novel deep learning-based methods to speech feature analysis. Finally, we will present our deep learning approaches to speaker identification and speech to face synthesis. The presented method can convert a speaker audio sample to an image of their predicted face. This framework is composed of several chained together networks, each with an essential step in the conversion process. These include Audio embedding, encoding, and face generation networks, respectively. Our experiments show that certain features can map to the face and that with a speaker\\u27s voice, DNNs can create their face and that a GUI could be used in conjunction to display a speaker recognition network\\u27s data",
      "word_count": 193,
      "core_id": 68969929,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.512972"
    },
    {
      "title": "Using computer vision in security applications",
      "abstract": "In this paper we present projects developed in the Computer Vision Laboratory, which address the issue of safety. First, we present the Internet Video Server (IVS) monitoring system [5] that sends live video stream over the Internet and enables remote camera control. Its extension GlobalView [1,6], which incorporates intuitive user interface for remote camera control, is based on panoramic image. Then we describe our method for automatic face detection [3] based on color segmentation and feature extraction. Finally, we introduce our SecurityAgent system [4] for automatic surveillance of observed location",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "In this paper we present projects developed in the Computer Vision Laboratory, which address the issue of safety. First, we present the Internet Video Server (IVS) monitoring system [5] that sends live video stream over the Internet and enables remote camera control. Its extension GlobalView [1,6], which incorporates intuitive user interface for remote camera control, is based on panoramic image. Then we describe our method for automatic face detection [3] based on color segmentation and feature extraction. Finally, we introduce our SecurityAgent system [4] for automatic surveillance of observed location",
      "word_count": 90,
      "core_id": 4960846,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513003"
    },
    {
      "title": "Cost-effective HPC clustering for computer vision applications",
      "abstract": "We will present a cost-effective and flexible realization of high performance computing (HPC) clustering and its potential in solving computationally intensive problems in computer vision. The featured software foundation to support the parallel programming is the GNU parallel Knoppix package with message passing interface (MPI) based Octave, Python and C interface capabilities. The implementation is especially of interest in applications where the main objective is to reuse the existing hardware infrastructure and to maintain the overall budget cost. We will present the benchmark results and compare and contrast the performances of Octave and MATLAB",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "We will present a cost-effective and flexible realization of high performance computing (HPC) clustering and its potential in solving computationally intensive problems in computer vision. The featured software foundation to support the parallel programming is the GNU parallel Knoppix package with message passing interface (MPI) based Octave, Python and C interface capabilities. The implementation is especially of interest in applications where the main objective is to reuse the existing hardware infrastructure and to maintain the overall budget cost. We will present the benchmark results and compare and contrast the performances of Octave and MATLAB",
      "word_count": 94,
      "core_id": 4763202,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513032"
    },
    {
      "title": "Efficient Clustering on Riemannian Manifolds: A Kernelised Random\n  Projection Approach",
      "abstract": "Reformulating computer vision problems over Riemannian manifolds has\ndemonstrated superior performance in various computer vision applications. This\nis because visual data often forms a special structure lying on a lower\ndimensional space embedded in a higher dimensional space. However, since these\nmanifolds belong to non-Euclidean topological spaces, exploiting their\nstructures is computationally expensive, especially when one considers the\nclustering analysis of massive amounts of data. To this end, we propose an\nefficient framework to address the clustering problem on Riemannian manifolds.\nThis framework implements random projections for manifold points via kernel\nspace, which can preserve the geometric structure of the original space, but is\ncomputationally efficient. Here, we introduce three methods that follow our\nframework. We then validate our framework on several computer vision\napplications by comparing against popular clustering methods on Riemannian\nmanifolds. Experimental results demonstrate that our framework maintains the\nperformance of the clustering whilst massively reducing computational\ncomplexity by over two orders of magnitude in some cases",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "Reformulating computer vision problems over Riemannian manifolds has\ndemonstrated superior performance in various computer vision applications. This\nis because visual data often forms a special structure lying on a lower\ndimensional space embedded in a higher dimensional space. However, since these\nmanifolds belong to non-Euclidean topological spaces, exploiting their\nstructures is computationally expensive, especially when one considers the\nclustering analysis of massive amounts of data. To this end, we propose an\nefficient framework to address the clustering problem on Riemannian manifolds.\nThis framework implements random projections for manifold points via kernel\nspace, which can preserve the geometric structure of the original space, but is\ncomputationally efficient. Here, we introduce three methods that follow our\nframework. We then validate our framework on several computer vision\napplications by comparing against popular clustering methods on Riemannian\nmanifolds. Experimental results demonstrate that our framework maintains the\nperformance of the clustering whilst massively reducing computational\ncomplexity by over two orders of magnitude in some cases",
      "word_count": 160,
      "core_id": 24729697,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513057"
    },
    {
      "title": "A Taxonomy of Deep Convolutional Neural Nets for Computer Vision",
      "abstract": "Traditional architectures for solving computer vision problems and the degree\nof success they enjoyed have been heavily reliant on hand-crafted features.\nHowever, of late, deep learning techniques have offered a compelling\nalternative -- that of automatically learning problem-specific features. With\nthis new paradigm, every problem in computer vision is now being re-examined\nfrom a deep learning perspective. Therefore, it has become important to\nunderstand what kind of deep networks are suitable for a given problem.\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\nexist, a survey specific to computer vision is missing. We specifically\nconsider one form of deep networks widely used in computer vision -\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\nand then examine the broad variations proposed over time to suit different\napplications. We hope that our recipe-style survey will serve as a guide,\nparticularly for novice practitioners intending to use deep-learning techniques\nfor computer vision.Comment: Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm",
      "keywords": [],
      "paper_type": "technical",
      "field": "computer_science",
      "content": "Traditional architectures for solving computer vision problems and the degree\nof success they enjoyed have been heavily reliant on hand-crafted features.\nHowever, of late, deep learning techniques have offered a compelling\nalternative -- that of automatically learning problem-specific features. With\nthis new paradigm, every problem in computer vision is now being re-examined\nfrom a deep learning perspective. Therefore, it has become important to\nunderstand what kind of deep networks are suitable for a given problem.\nAlthough general surveys of this fast-moving paradigm (i.e. deep-networks)\nexist, a survey specific to computer vision is missing. We specifically\nconsider one form of deep networks widely used in computer vision -\nconvolutional neural networks (CNNs). We start with \"AlexNet\" as our base CNN\nand then examine the broad variations proposed over time to suit different\napplications. We hope that our recipe-style survey will serve as a guide,\nparticularly for novice practitioners intending to use deep-learning techniques\nfor computer vision.Comment: Published in Frontiers in Robotics and AI (http://goo.gl/6691Bm",
      "word_count": 163,
      "core_id": 24761998,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513097"
    },
    {
      "title": "A Simple and Correct Even-Odd Algorithm for the Point-in-Polygon Problem\n  for Complex Polygons",
      "abstract": "Determining if a point is in a polygon or not is used by a lot of\napplications in computer graphics, computer games and geoinformatics.\nImplementing this check is error-prone since there are many special cases to be\nconsidered. This holds true in particular for complex polygons whose edges\nintersect each other creating holes. In this paper we present a simple even-odd\nalgorithm to solve this problem for complex polygons in linear time and prove\nits correctness for all possible points and polygons. We furthermore provide\nexamples and implementation notes for this algorithm.Comment: Proceedings of the 12th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP\n  2017), Volume 1: GRAP",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "Determining if a point is in a polygon or not is used by a lot of\napplications in computer graphics, computer games and geoinformatics.\nImplementing this check is error-prone since there are many special cases to be\nconsidered. This holds true in particular for complex polygons whose edges\nintersect each other creating holes. In this paper we present a simple even-odd\nalgorithm to solve this problem for complex polygons in linear time and prove\nits correctness for all possible points and polygons. We furthermore provide\nexamples and implementation notes for this algorithm.Comment: Proceedings of the 12th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP\n  2017), Volume 1: GRAP",
      "word_count": 114,
      "core_id": 4085792,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513114"
    },
    {
      "title": "Many-to-Many Graph Matching: a Continuous Relaxation Approach",
      "abstract": "Graphs provide an efficient tool for object representation in various\ncomputer vision applications. Once graph-based representations are constructed,\nan important question is how to compare graphs. This problem is often\nformulated as a graph matching problem where one seeks a mapping between\nvertices of two graphs which optimally aligns their structure. In the classical\nformulation of graph matching, only one-to-one correspondences between vertices\nare considered. However, in many applications, graphs cannot be matched\nperfectly and it is more interesting to consider many-to-many correspondences\nwhere clusters of vertices in one graph are matched to clusters of vertices in\nthe other graph. In this paper, we formulate the many-to-many graph matching\nproblem as a discrete optimization problem and propose an approximate algorithm\nbased on a continuous relaxation of the combinatorial problem. We compare our\nmethod with other existing methods on several benchmark computer vision\ndatasets.Comment: 1",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Graphs provide an efficient tool for object representation in various\ncomputer vision applications. Once graph-based representations are constructed,\nan important question is how to compare graphs. This problem is often\nformulated as a graph matching problem where one seeks a mapping between\nvertices of two graphs which optimally aligns their structure. In the classical\nformulation of graph matching, only one-to-one correspondences between vertices\nare considered. However, in many applications, graphs cannot be matched\nperfectly and it is more interesting to consider many-to-many correspondences\nwhere clusters of vertices in one graph are matched to clusters of vertices in\nthe other graph. In this paper, we formulate the many-to-many graph matching\nproblem as a discrete optimization problem and propose an approximate algorithm\nbased on a continuous relaxation of the combinatorial problem. We compare our\nmethod with other existing methods on several benchmark computer vision\ndatasets.Comment: 1",
      "word_count": 144,
      "core_id": 696938,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513143"
    },
    {
      "title": "Estimating the Potential Speedup of Computer Vision Applications on\n  Embedded Multiprocessors",
      "abstract": "Computer vision applications constitute one of the key drivers for embedded\nmulticore architectures. Although the number of available cores is increasing\nin new architectures, designing an application to maximize the utilization of\nthe platform is still a challenge. In this sense, parallel performance\nprediction tools can aid developers in understanding the characteristics of an\napplication and finding the most adequate parallelization strategy. In this\nwork, we present a method for early parallel performance estimation on embedded\nmultiprocessors from sequential application traces. We describe its\nimplementation in Parana, a fast trace-driven simulator targeting OpenMP\napplications on the STMicroelectronics' STxP70 Application-Specific\nMultiprocessor (ASMP). Results for the FAST key point detector application show\nan error margin of less than 10% compared to the reference cycle-approximate\nsimulator, with lower modeling effort and up to 20x faster execution time.Comment: Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241",
      "keywords": [],
      "paper_type": "comparative",
      "field": "computer_science",
      "content": "Computer vision applications constitute one of the key drivers for embedded\nmulticore architectures. Although the number of available cores is increasing\nin new architectures, designing an application to maximize the utilization of\nthe platform is still a challenge. In this sense, parallel performance\nprediction tools can aid developers in understanding the characteristics of an\napplication and finding the most adequate parallelization strategy. In this\nwork, we present a method for early parallel performance estimation on embedded\nmultiprocessors from sequential application traces. We describe its\nimplementation in Parana, a fast trace-driven simulator targeting OpenMP\napplications on the STMicroelectronics' STxP70 Application-Specific\nMultiprocessor (ASMP). Results for the FAST key point detector application show\nan error margin of less than 10% compared to the reference cycle-approximate\nsimulator, with lower modeling effort and up to 20x faster execution time.Comment: Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241",
      "word_count": 152,
      "core_id": 18048061,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513173"
    },
    {
      "title": "Combining logic and probability in tracking and scene interpretation",
      "abstract": "The paper gives a high-level overview of some ways in which logical representations and reasoning can be used in computer vision applications, such as tracking and scene interpretation. The combination of logical and statistical approaches is also considered",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "The paper gives a high-level overview of some ways in which logical representations and reasoning can be used in computer vision applications, such as tracking and scene interpretation. The combination of logical and statistical approaches is also considered",
      "word_count": 38,
      "core_id": 233961,
      "year": null,
      "processed_date": "2025-07-01T10:45:40.513187"
    },
    {
      "title": "Reinforcement Learning: A Survey",
      "abstract": "This paper surveys the field of reinforcement learning from a\ncomputer-science perspective. It is written to be accessible to researchers\nfamiliar with machine learning. Both the historical basis of the field and a\nbroad selection of current work are summarized. Reinforcement learning is the\nproblem faced by an agent that learns behavior through trial-and-error\ninteractions with a dynamic environment. The work described here has a\nresemblance to work in psychology, but differs considerably in the details and\nin the use of the word ``reinforcement.'' The paper discusses central issues of\nreinforcement learning, including trading off exploration and exploitation,\nestablishing the foundations of the field via Markov decision theory, learning\nfrom delayed reinforcement, constructing empirical models to accelerate\nlearning, making use of generalization and hierarchy, and coping with hidden\nstate. It concludes with a survey of some implemented systems and an assessment\nof the practical utility of current methods for reinforcement learning.Comment: See http://www.jair.org/ for any accompanying file",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "social_sciences",
      "content": "This paper surveys the field of reinforcement learning from a\ncomputer-science perspective. It is written to be accessible to researchers\nfamiliar with machine learning. Both the historical basis of the field and a\nbroad selection of current work are summarized. Reinforcement learning is the\nproblem faced by an agent that learns behavior through trial-and-error\ninteractions with a dynamic environment. The work described here has a\nresemblance to work in psychology, but differs considerably in the details and\nin the use of the word ``reinforcement.'' The paper discusses central issues of\nreinforcement learning, including trading off exploration and exploitation,\nestablishing the foundations of the field via Markov decision theory, learning\nfrom delayed reinforcement, constructing empirical models to accelerate\nlearning, making use of generalization and hierarchy, and coping with hidden\nstate. It concludes with a survey of some implemented systems and an assessment\nof the practical utility of current methods for reinforcement learning.Comment: See http://www.jair.org/ for any accompanying file",
      "word_count": 157,
      "core_id": 947972,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733027"
    },
    {
      "title": "Stochastic Reinforcement Learning",
      "abstract": "In reinforcement learning episodes, the rewards and punishments are often\nnon-deterministic, and there are invariably stochastic elements governing the\nunderlying situation. Such stochastic elements are often numerous and cannot be\nknown in advance, and they have a tendency to obscure the underlying rewards\nand punishments patterns. Indeed, if stochastic elements were absent, the same\noutcome would occur every time and the learning problems involved could be\ngreatly simplified. In addition, in most practical situations, the cost of an\nobservation to receive either a reward or punishment can be significant, and\none would wish to arrive at the correct learning conclusion by incurring\nminimum cost. In this paper, we present a stochastic approach to reinforcement\nlearning which explicitly models the variability present in the learning\nenvironment and the cost of observation. Criteria and rules for learning\nsuccess are quantitatively analyzed, and probabilities of exceeding the\nobservation cost bounds are also obtained.Comment: AIKE 201",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "In reinforcement learning episodes, the rewards and punishments are often\nnon-deterministic, and there are invariably stochastic elements governing the\nunderlying situation. Such stochastic elements are often numerous and cannot be\nknown in advance, and they have a tendency to obscure the underlying rewards\nand punishments patterns. Indeed, if stochastic elements were absent, the same\noutcome would occur every time and the learning problems involved could be\ngreatly simplified. In addition, in most practical situations, the cost of an\nobservation to receive either a reward or punishment can be significant, and\none would wish to arrive at the correct learning conclusion by incurring\nminimum cost. In this paper, we present a stochastic approach to reinforcement\nlearning which explicitly models the variability present in the learning\nenvironment and the cost of observation. Criteria and rules for learning\nsuccess are quantitatively analyzed, and probabilities of exceeding the\nobservation cost bounds are also obtained.Comment: AIKE 201",
      "word_count": 152,
      "core_id": 58950127,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733067"
    },
    {
      "title": "Atari games and Intel processors",
      "abstract": "The asynchronous nature of the state-of-the-art reinforcement learning\nalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes\nthem exceptionally suitable for CPU computations. However, given the fact that\ndeep reinforcement learning often deals with interpreting visual information, a\nlarge part of the train and inference time is spent performing convolutions. In\nthis work we present our results on learning strategies in Atari games using a\nConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0\nmachine learning framework. We also analyze effects of asynchronous\ncomputations on the convergence of reinforcement learning algorithms",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "computer_science",
      "content": "The asynchronous nature of the state-of-the-art reinforcement learning\nalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes\nthem exceptionally suitable for CPU computations. However, given the fact that\ndeep reinforcement learning often deals with interpreting visual information, a\nlarge part of the train and inference time is spent performing convolutions. In\nthis work we present our results on learning strategies in Atari games using a\nConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0\nmachine learning framework. We also analyze effects of asynchronous\ncomputations on the convergence of reinforcement learning algorithms",
      "word_count": 92,
      "core_id": 42867319,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733087"
    },
    {
      "title": "Virtual to Real Reinforcement Learning for Autonomous Driving",
      "abstract": "Reinforcement learning is considered as a promising direction for driving\npolicy learning. However, training autonomous driving vehicle with\nreinforcement learning in real environment involves non-affordable\ntrial-and-error. It is more desirable to first train in a virtual environment\nand then transfer to the real environment. In this paper, we propose a novel\nrealistic translation network to make model trained in virtual environment be\nworkable in real world. The proposed network can convert non-realistic virtual\nimage input into a realistic one with similar scene structure. Given realistic\nframes as input, driving policy trained by reinforcement learning can nicely\nadapt to real world driving. Experiments show that our proposed virtual to real\n(VR) reinforcement learning (RL) works pretty well. To our knowledge, this is\nthe first successful case of driving policy trained by reinforcement learning\nthat can adapt to real world driving data",
      "keywords": [],
      "paper_type": "empirical",
      "field": "computer_science",
      "content": "Reinforcement learning is considered as a promising direction for driving\npolicy learning. However, training autonomous driving vehicle with\nreinforcement learning in real environment involves non-affordable\ntrial-and-error. It is more desirable to first train in a virtual environment\nand then transfer to the real environment. In this paper, we propose a novel\nrealistic translation network to make model trained in virtual environment be\nworkable in real world. The proposed network can convert non-realistic virtual\nimage input into a realistic one with similar scene structure. Given realistic\nframes as input, driving policy trained by reinforcement learning can nicely\nadapt to real world driving. Experiments show that our proposed virtual to real\n(VR) reinforcement learning (RL) works pretty well. To our knowledge, this is\nthe first successful case of driving policy trained by reinforcement learning\nthat can adapt to real world driving data",
      "word_count": 140,
      "core_id": 42856612,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733126"
    },
    {
      "title": "Selection in Scale-Free Small World",
      "abstract": "In this paper we compare the performance characteristics of our selection\nbased learning algorithm for Web crawlers with the characteristics of the\nreinforcement learning algorithm. The task of the crawlers is to find new\ninformation on the Web. The selection algorithm, called weblog update, modifies\nthe starting URL lists of our crawlers based on the found URLs containing new\ninformation. The reinforcement learning algorithm modifies the URL orderings of\nthe crawlers based on the received reinforcements for submitted documents. We\nperformed simulations based on data collected from the Web. The collected\nportion of the Web is typical and exhibits scale-free small world (SFSW)\nstructure. We have found that on this SFSW, the weblog update algorithm\nperforms better than the reinforcement learning algorithm. It finds the new\ninformation faster than the reinforcement learning algorithm and has better new\ninformation/all submitted documents ratio. We believe that the advantages of\nthe selection algorithm over reinforcement learning algorithm is due to the\nsmall world property of the Web.Comment: 24 pages, 3 figure",
      "keywords": [],
      "paper_type": "empirical",
      "field": "computer_science",
      "content": "In this paper we compare the performance characteristics of our selection\nbased learning algorithm for Web crawlers with the characteristics of the\nreinforcement learning algorithm. The task of the crawlers is to find new\ninformation on the Web. The selection algorithm, called weblog update, modifies\nthe starting URL lists of our crawlers based on the found URLs containing new\ninformation. The reinforcement learning algorithm modifies the URL orderings of\nthe crawlers based on the received reinforcements for submitted documents. We\nperformed simulations based on data collected from the Web. The collected\nportion of the Web is typical and exhibits scale-free small world (SFSW)\nstructure. We have found that on this SFSW, the weblog update algorithm\nperforms better than the reinforcement learning algorithm. It finds the new\ninformation faster than the reinforcement learning algorithm and has better new\ninformation/all submitted documents ratio. We believe that the advantages of\nthe selection algorithm over reinforcement learning algorithm is due to the\nsmall world property of the Web.Comment: 24 pages, 3 figure",
      "word_count": 168,
      "core_id": 945491,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733167"
    },
    {
      "title": "Reinforcement learning in populations of spiking neurons",
      "abstract": "Population coding is widely regarded as a key mechanism for achieving reliable behavioral responses in the face of neuronal variability. But in standard reinforcement learning a flip-side becomes apparent. Learning slows down with increasing population size since the global reinforcement becomes less and less related to the performance of any single neuron. We show that, in contrast, learning speeds up with increasing population size if feedback about the populationresponse modulates synaptic plasticity in addition to global reinforcement. The two feedback signals (reinforcement and population-response signal) can be encoded by ambient neurotransmitter concentrations which vary slowly, yielding a fully online plasticity rule where the learning of a stimulus is interleaved with the processing of the subsequent one. The assumption of a single additional feedback mechanism therefore reconciles biological plausibility with efficient learning",
      "keywords": [],
      "paper_type": "empirical",
      "field": "social_sciences",
      "content": "Population coding is widely regarded as a key mechanism for achieving reliable behavioral responses in the face of neuronal variability. But in standard reinforcement learning a flip-side becomes apparent. Learning slows down with increasing population size since the global reinforcement becomes less and less related to the performance of any single neuron. We show that, in contrast, learning speeds up with increasing population size if feedback about the populationresponse modulates synaptic plasticity in addition to global reinforcement. The two feedback signals (reinforcement and population-response signal) can be encoded by ambient neurotransmitter concentrations which vary slowly, yielding a fully online plasticity rule where the learning of a stimulus is interleaved with the processing of the subsequent one. The assumption of a single additional feedback mechanism therefore reconciles biological plausibility with efficient learning",
      "word_count": 131,
      "core_id": 313364,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733200"
    },
    {
      "title": "Learning Strict Nash Equilibria through Reinforcement",
      "abstract": "This paper studies the analytical properties of the reinforcement learning model proposed in Erev and Roth (1998), also termed cumulative reinforcement learning in Laslier et al (2001). This stochastic model of learning in games accounts for two main elements: the law of effect (positive reinforcement of actions that perform well) and the law of practice (the magnitude of the reinforcement effect decreases with players' experience). The main results of the paper show that, if the solution trajectories of the underlying replicator equation converge exponentially fast, then, with probability arbitrarily close to one, all the realizations of the reinforcement learning process will, from some time on, lie within an \" band of that solution. The paper improves upon results currently available in the literature by showing that a reinforcement learning process that has been running for some time and is found suffciently close to a strict Nash equilibrium, will reach it with probability one.Strict Nash Equilibrium, Reinforcement Learning",
      "keywords": [],
      "paper_type": "review",
      "field": "computer_science",
      "content": "This paper studies the analytical properties of the reinforcement learning model proposed in Erev and Roth (1998), also termed cumulative reinforcement learning in Laslier et al (2001). This stochastic model of learning in games accounts for two main elements: the law of effect (positive reinforcement of actions that perform well) and the law of practice (the magnitude of the reinforcement effect decreases with players' experience). The main results of the paper show that, if the solution trajectories of the underlying replicator equation converge exponentially fast, then, with probability arbitrarily close to one, all the realizations of the reinforcement learning process will, from some time on, lie within an \" band of that solution. The paper improves upon results currently available in the literature by showing that a reinforcement learning process that has been running for some time and is found suffciently close to a strict Nash equilibrium, will reach it with probability one.Strict Nash Equilibrium, Reinforcement Learning",
      "word_count": 157,
      "core_id": 2720276,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733218"
    },
    {
      "title": "The Dreaming Variational Autoencoder for Reinforcement Learning\n  Environments",
      "abstract": "Reinforcement learning has shown great potential in generalizing over raw\nsensory data using only a single neural network for value optimization. There\nare several challenges in the current state-of-the-art reinforcement learning\nalgorithms that prevent them from converging towards the global optima. It is\nlikely that the solution to these problems lies in short- and long-term\nplanning, exploration and memory management for reinforcement learning\nalgorithms. Games are often used to benchmark reinforcement learning algorithms\nas they provide a flexible, reproducible, and easy to control environment.\nRegardless, few games feature a state-space where results in exploration,\nmemory, and planning are easily perceived. This paper presents The Dreaming\nVariational Autoencoder (DVAE), a neural network based generative modeling\narchitecture for exploration in environments with sparse feedback. We further\npresent Deep Maze, a novel and flexible maze engine that challenges DVAE in\npartial and fully-observable state-spaces, long-horizon tasks, and\ndeterministic and stochastic problems. We show initial findings and encourage\nfurther work in reinforcement learning driven by generative exploration.Comment: Best Student Paper Award, Proceedings of the 38th SGAI International\n  Conference on Artificial Intelligence, Cambridge, UK, 2018, Artificial\n  Intelligence XXXV, 201",
      "keywords": [],
      "paper_type": "technical",
      "field": "business",
      "content": "Reinforcement learning has shown great potential in generalizing over raw\nsensory data using only a single neural network for value optimization. There\nare several challenges in the current state-of-the-art reinforcement learning\nalgorithms that prevent them from converging towards the global optima. It is\nlikely that the solution to these problems lies in short- and long-term\nplanning, exploration and memory management for reinforcement learning\nalgorithms. Games are often used to benchmark reinforcement learning algorithms\nas they provide a flexible, reproducible, and easy to control environment.\nRegardless, few games feature a state-space where results in exploration,\nmemory, and planning are easily perceived. This paper presents The Dreaming\nVariational Autoencoder (DVAE), a neural network based generative modeling\narchitecture for exploration in environments with sparse feedback. We further\npresent Deep Maze, a novel and flexible maze engine that challenges DVAE in\npartial and fully-observable state-spaces, long-horizon tasks, and\ndeterministic and stochastic problems. We show initial findings and encourage\nfurther work in reinforcement learning driven by generative exploration.Comment: Best Student Paper Award, Proceedings of the 38th SGAI International\n  Conference on Artificial Intelligence, Cambridge, UK, 2018, Artificial\n  Intelligence XXXV, 201",
      "word_count": 184,
      "core_id": 54157810,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733262"
    },
    {
      "title": "Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning",
      "abstract": "Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters",
      "word_count": 131,
      "core_id": 78116686,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733293"
    },
    {
      "title": "Stochastic Inverse Reinforcement Learning",
      "abstract": "The goal of the inverse reinforcement learning (IRL) problem is to recover\nthe reward functions from expert demonstrations. However, the IRL problem like\nany ill-posed inverse problem suffers the congenital defect that the policy may\nbe optimal for many reward functions, and expert demonstrations may be optimal\nfor many policies. In this work, we generalize the IRL problem to a well-posed\nexpectation optimization problem stochastic inverse reinforcement learning\n(SIRL) to recover the probability distribution over reward functions. We adopt\nthe Monte Carlo expectation-maximization (MCEM) method to estimate the\nparameter of the probability distribution as the first solution to the SIRL\nproblem. The solution is succinct, robust, and transferable for a learning task\nand can generate alternative solutions to the IRL problem. Through our\nformulation, it is possible to observe the intrinsic property for the IRL\nproblem from a global viewpoint, and our approach achieves a considerable\nperformance on the objectworld.Comment: 8+2 pages, 5 figures, Under Revie",
      "keywords": [],
      "paper_type": "methodological",
      "field": "computer_science",
      "content": "The goal of the inverse reinforcement learning (IRL) problem is to recover\nthe reward functions from expert demonstrations. However, the IRL problem like\nany ill-posed inverse problem suffers the congenital defect that the policy may\nbe optimal for many reward functions, and expert demonstrations may be optimal\nfor many policies. In this work, we generalize the IRL problem to a well-posed\nexpectation optimization problem stochastic inverse reinforcement learning\n(SIRL) to recover the probability distribution over reward functions. We adopt\nthe Monte Carlo expectation-maximization (MCEM) method to estimate the\nparameter of the probability distribution as the first solution to the SIRL\nproblem. The solution is succinct, robust, and transferable for a learning task\nand can generate alternative solutions to the IRL problem. Through our\nformulation, it is possible to observe the intrinsic property for the IRL\nproblem from a global viewpoint, and our approach achieves a considerable\nperformance on the objectworld.Comment: 8+2 pages, 5 figures, Under Revie",
      "word_count": 156,
      "core_id": 58979620,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733322"
    },
    {
      "title": "Fine-grained acceleration control for autonomous intersection management\n  using deep reinforcement learning",
      "abstract": "Recent advances in combining deep learning and Reinforcement Learning have\nshown a promising path for designing new control agents that can learn optimal\npolicies for challenging control tasks. These new methods address the main\nlimitations of conventional Reinforcement Learning methods such as customized\nfeature engineering and small action/state space dimension requirements. In\nthis paper, we leverage one of the state-of-the-art Reinforcement Learning\nmethods, known as Trust Region Policy Optimization, to tackle intersection\nmanagement for autonomous vehicles. We show that using this method, we can\nperform fine-grained acceleration control of autonomous vehicles in a grid\nstreet plan to achieve a global design objective.Comment: Accepted in IEEE Smart World Congress 201",
      "keywords": [],
      "paper_type": "methodological",
      "field": "business",
      "content": "Recent advances in combining deep learning and Reinforcement Learning have\nshown a promising path for designing new control agents that can learn optimal\npolicies for challenging control tasks. These new methods address the main\nlimitations of conventional Reinforcement Learning methods such as customized\nfeature engineering and small action/state space dimension requirements. In\nthis paper, we leverage one of the state-of-the-art Reinforcement Learning\nmethods, known as Trust Region Policy Optimization, to tackle intersection\nmanagement for autonomous vehicles. We show that using this method, we can\nperform fine-grained acceleration control of autonomous vehicles in a grid\nstreet plan to achieve a global design objective.Comment: Accepted in IEEE Smart World Congress 201",
      "word_count": 109,
      "core_id": 42870080,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733351"
    },
    {
      "title": "Modelling Emotion Based Reward Valuation with Computational Reinforcement Learning",
      "abstract": "We show that computational reinforcement learning can model human decision making in the Iowa Gambling Task (IGT). The IGT is a card game, which tests decision making under uncertainty. In our experiments, we found that modulating learning rate decay in Q-learning, enables the approximation of both the behaviour of normal subjects and those who are emotionally impaired by ventromedial prefrontal lesions. Outcomes observed in impaired subjects are modeled by high learning rate decay, while low learning rate decay replicates healthy subjects under otherwise identical conditions. The ventromedial prefrontal cortex has been associated with emotion based reward valuation, and, the value function in reinforcement learning provides an analogous assessment mechanism. Thus reinforcement learning can provide a good model for the role of emotional reward as a modulator of the learning rate",
      "keywords": [],
      "paper_type": "empirical",
      "field": "medicine",
      "content": "We show that computational reinforcement learning can model human decision making in the Iowa Gambling Task (IGT). The IGT is a card game, which tests decision making under uncertainty. In our experiments, we found that modulating learning rate decay in Q-learning, enables the approximation of both the behaviour of normal subjects and those who are emotionally impaired by ventromedial prefrontal lesions. Outcomes observed in impaired subjects are modeled by high learning rate decay, while low learning rate decay replicates healthy subjects under otherwise identical conditions. The ventromedial prefrontal cortex has been associated with emotion based reward valuation, and, the value function in reinforcement learning provides an analogous assessment mechanism. Thus reinforcement learning can provide a good model for the role of emotional reward as a modulator of the learning rate",
      "word_count": 130,
      "core_id": 12265079,
      "year": null,
      "processed_date": "2025-07-01T10:45:41.733380"
    },
    {
      "title": "Beyond Volume: The Impact of Complex Healthcare Data on the Machine\n  Learning Pipeline",
      "abstract": "From medical charts to national census, healthcare has traditionally operated\nunder a paper-based paradigm. However, the past decade has marked a long and\narduous transformation bringing healthcare into the digital age. Ranging from\nelectronic health records, to digitized imaging and laboratory reports, to\npublic health datasets, today, healthcare now generates an incredible amount of\ndigital information. Such a wealth of data presents an exciting opportunity for\nintegrated machine learning solutions to address problems across multiple\nfacets of healthcare practice and administration. Unfortunately, the ability to\nderive accurate and informative insights requires more than the ability to\nexecute machine learning models. Rather, a deeper understanding of the data on\nwhich the models are run is imperative for their success. While a significant\neffort has been undertaken to develop models able to process the volume of data\nobtained during the analysis of millions of digitalized patient records, it is\nimportant to remember that volume represents only one aspect of the data. In\nfact, drawing on data from an increasingly diverse set of sources, healthcare\ndata presents an incredibly complex set of attributes that must be accounted\nfor throughout the machine learning pipeline. This chapter focuses on\nhighlighting such challenges, and is broken down into three distinct\ncomponents, each representing a phase of the pipeline. We begin with attributes\nof the data accounted for during preprocessing, then move to considerations\nduring model building, and end with challenges to the interpretation of model\noutput. For each component, we present a discussion around data as it relates\nto the healthcare domain and offer insight into the challenges each may impose\non the efficiency of machine learning techniques.Comment: Healthcare Informatics, Machine Learning, Knowledge Discovery: 20\n  Pages, 1 Figur",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "From medical charts to national census, healthcare has traditionally operated\nunder a paper-based paradigm. However, the past decade has marked a long and\narduous transformation bringing healthcare into the digital age. Ranging from\nelectronic health records, to digitized imaging and laboratory reports, to\npublic health datasets, today, healthcare now generates an incredible amount of\ndigital information. Such a wealth of data presents an exciting opportunity for\nintegrated machine learning solutions to address problems across multiple\nfacets of healthcare practice and administration. Unfortunately, the ability to\nderive accurate and informative insights requires more than the ability to\nexecute machine learning models. Rather, a deeper understanding of the data on\nwhich the models are run is imperative for their success. While a significant\neffort has been undertaken to develop models able to process the volume of data\nobtained during the analysis of millions of digitalized patient records, it is\nimportant to remember that volume represents only one aspect of the data. In\nfact, drawing on data from an increasingly diverse set of sources, healthcare\ndata presents an incredibly complex set of attributes that must be accounted\nfor throughout the machine learning pipeline. This chapter focuses on\nhighlighting such challenges, and is broken down into three distinct\ncomponents, each representing a phase of the pipeline. We begin with attributes\nof the data accounted for during preprocessing, then move to considerations\nduring model building, and end with challenges to the interpretation of model\noutput. For each component, we present a discussion around data as it relates\nto the healthcare domain and offer insight into the challenges each may impose\non the efficiency of machine learning techniques.Comment: Healthcare Informatics, Machine Learning, Knowledge Discovery: 20\n  Pages, 1 Figur",
      "word_count": 282,
      "core_id": 42871850,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321707"
    },
    {
      "title": "Privacy-preserving scoring of tree ensembles : a novel framework for AI in healthcare",
      "abstract": "Machine Learning (ML) techniques now impact a wide variety of domains. Highly regulated industries such as healthcare and finance have stringent compliance and data governance policies around data sharing. Advances in secure multiparty computation (SMC) for privacy-preserving machine learning (PPML) can help transform these regulated industries by allowing ML computations over encrypted data with personally identifiable information (PII). Yet very little of SMC-based PPML has been put into practice so far. In this paper we present the very first framework for privacy-preserving classification of tree ensembles with application in healthcare. We first describe the underlying cryptographic protocols that enable a healthcare organization to send encrypted data securely to a ML scoring service and obtain encrypted class labels without the scoring service actually seeing that input in the clear. We then describe the deployment challenges we solved to integrate these protocols in a cloud based scalable risk-prediction platform with multiple ML models for healthcare AI. Included are system internals, and evaluations of our deployment for supporting physicians to drive better clinical outcomes in an accurate, scalable, and provably secure manner. To the best of our knowledge, this is the first such applied framework with SMC-based privacy-preserving machine learning for healthcare",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "medicine",
      "content": "Machine Learning (ML) techniques now impact a wide variety of domains. Highly regulated industries such as healthcare and finance have stringent compliance and data governance policies around data sharing. Advances in secure multiparty computation (SMC) for privacy-preserving machine learning (PPML) can help transform these regulated industries by allowing ML computations over encrypted data with personally identifiable information (PII). Yet very little of SMC-based PPML has been put into practice so far. In this paper we present the very first framework for privacy-preserving classification of tree ensembles with application in healthcare. We first describe the underlying cryptographic protocols that enable a healthcare organization to send encrypted data securely to a ML scoring service and obtain encrypted class labels without the scoring service actually seeing that input in the clear. We then describe the deployment challenges we solved to integrate these protocols in a cloud based scalable risk-prediction platform with multiple ML models for healthcare AI. Included are system internals, and evaluations of our deployment for supporting physicians to drive better clinical outcomes in an accurate, scalable, and provably secure manner. To the best of our knowledge, this is the first such applied framework with SMC-based privacy-preserving machine learning for healthcare",
      "word_count": 199,
      "core_id": 71793392,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321732"
    },
    {
      "title": "Processing of Electronic Health Records using Deep Learning: A review",
      "abstract": "Availability of large amount of clinical data is opening up new research\navenues in a number of fields. An exciting field in this respect is healthcare,\nwhere secondary use of healthcare data is beginning to revolutionize\nhealthcare. Except for availability of Big Data, both medical data from\nhealthcare institutions (such as EMR data) and data generated from health and\nwellbeing devices (such as personal trackers), a significant contribution to\nthis trend is also being made by recent advances on machine learning,\nspecifically deep learning algorithms",
      "keywords": [],
      "paper_type": "review",
      "field": "medicine",
      "content": "Availability of large amount of clinical data is opening up new research\navenues in a number of fields. An exciting field in this respect is healthcare,\nwhere secondary use of healthcare data is beginning to revolutionize\nhealthcare. Except for availability of Big Data, both medical data from\nhealthcare institutions (such as EMR data) and data generated from health and\nwellbeing devices (such as personal trackers), a significant contribution to\nthis trend is also being made by recent advances on machine learning,\nspecifically deep learning algorithms",
      "word_count": 85,
      "core_id": 50751960,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321755"
    },
    {
      "title": "Towards Deep Learning Models for Psychological State Prediction using\n  Smartphone Data: Challenges and Opportunities",
      "abstract": "There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction.Comment: 6 pages, 2 figures, In Proceedings of the NIPS Workshop on Machine\n  Learning for Healthcare 2017 (ML4H 2017). Colocated with NIPS 201",
      "keywords": [],
      "paper_type": "review",
      "field": "medicine",
      "content": "There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction.Comment: 6 pages, 2 figures, In Proceedings of the NIPS Workshop on Machine\n  Learning for Healthcare 2017 (ML4H 2017). Colocated with NIPS 201",
      "word_count": 146,
      "core_id": 45159799,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321767"
    },
    {
      "title": "Clustering Patients with Tensor Decomposition",
      "abstract": "In this paper we present a method for the unsupervised clustering of\nhigh-dimensional binary data, with a special focus on electronic healthcare\nrecords. We present a robust and efficient heuristic to face this problem using\ntensor decomposition. We present the reasons why this approach is preferable\nfor tasks such as clustering patient records, to more commonly used\ndistance-based methods. We run the algorithm on two datasets of healthcare\nrecords, obtaining clinically meaningful results.Comment: Presented at 2017 Machine Learning for Healthcare Conference (MLHC\n  2017). Boston, M",
      "keywords": [],
      "paper_type": "methodological",
      "field": "medicine",
      "content": "In this paper we present a method for the unsupervised clustering of\nhigh-dimensional binary data, with a special focus on electronic healthcare\nrecords. We present a robust and efficient heuristic to face this problem using\ntensor decomposition. We present the reasons why this approach is preferable\nfor tasks such as clustering patient records, to more commonly used\ndistance-based methods. We run the algorithm on two datasets of healthcare\nrecords, obtaining clinically meaningful results.Comment: Presented at 2017 Machine Learning for Healthcare Conference (MLHC\n  2017). Boston, M",
      "word_count": 85,
      "core_id": 44596541,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321782"
    },
    {
      "title": "Multimodal Machine Learning for Automated ICD Coding",
      "abstract": "This study presents a multimodal machine learning model to predict ICD-10\ndiagnostic codes. We developed separate machine learning models that can handle\ndata from different modalities, including unstructured text, semi-structured\ntext and structured tabular data. We further employed an ensemble method to\nintegrate all modality-specific models to generate ICD-10 codes. Key evidence\nwas also extracted to make our prediction more convincing and explainable. We\nused the Medical Information Mart for Intensive Care III (MIMIC -III) dataset\nto validate our approach. For ICD code prediction, our best-performing model\n(micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms other\nbaseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) and\nText-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability,\nour approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on text\ndata and 0.3105 on tabular data, where well-trained physicians achieve 0.2780\nand 0.5002 respectively.Comment: Machine Learning for Healthcare 201",
      "keywords": [],
      "paper_type": "methodological",
      "field": "medicine",
      "content": "This study presents a multimodal machine learning model to predict ICD-10\ndiagnostic codes. We developed separate machine learning models that can handle\ndata from different modalities, including unstructured text, semi-structured\ntext and structured tabular data. We further employed an ensemble method to\nintegrate all modality-specific models to generate ICD-10 codes. Key evidence\nwas also extracted to make our prediction more convincing and explainable. We\nused the Medical Information Mart for Intensive Care III (MIMIC -III) dataset\nto validate our approach. For ICD code prediction, our best-performing model\n(micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms other\nbaseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) and\nText-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability,\nour approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on text\ndata and 0.3105 on tabular data, where well-trained physicians achieve 0.2780\nand 0.5002 respectively.Comment: Machine Learning for Healthcare 201",
      "word_count": 148,
      "core_id": 54166446,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321804"
    },
    {
      "title": "Diagnostic Prediction Using Discomfort Drawings with IBTM",
      "abstract": "In this paper, we explore the possibility to apply machine learning to make\ndiagnostic predictions using discomfort drawings. A discomfort drawing is an\nintuitive way for patients to express discomfort and pain related symptoms.\nThese drawings have proven to be an effective method to collect patient data\nand make diagnostic decisions in real-life practice. A dataset from real-world\npatient cases is collected for which medical experts provide diagnostic labels.\nNext, we use a factorized multimodal topic model, Inter-Battery Topic Model\n(IBTM), to train a system that can make diagnostic predictions given an unseen\ndiscomfort drawing. The number of output diagnostic labels is determined by\nusing mean-shift clustering on the discomfort drawing. Experimental results\nshow reasonable predictions of diagnostic labels given an unseen discomfort\ndrawing. Additionally, we generate synthetic discomfort drawings with IBTM\ngiven a diagnostic label, which results in typical cases of symptoms. The\npositive result indicates a significant potential of machine learning to be\nused for parts of the pain diagnostic process and to be a decision support\nsystem for physicians and other health care personnel.Comment: Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, C",
      "keywords": [],
      "paper_type": "methodological",
      "field": "medicine",
      "content": "In this paper, we explore the possibility to apply machine learning to make\ndiagnostic predictions using discomfort drawings. A discomfort drawing is an\nintuitive way for patients to express discomfort and pain related symptoms.\nThese drawings have proven to be an effective method to collect patient data\nand make diagnostic decisions in real-life practice. A dataset from real-world\npatient cases is collected for which medical experts provide diagnostic labels.\nNext, we use a factorized multimodal topic model, Inter-Battery Topic Model\n(IBTM), to train a system that can make diagnostic predictions given an unseen\ndiscomfort drawing. The number of output diagnostic labels is determined by\nusing mean-shift clustering on the discomfort drawing. Experimental results\nshow reasonable predictions of diagnostic labels given an unseen discomfort\ndrawing. Additionally, we generate synthetic discomfort drawings with IBTM\ngiven a diagnostic label, which results in typical cases of symptoms. The\npositive result indicates a significant potential of machine learning to be\nused for parts of the pain diagnostic process and to be a decision support\nsystem for physicians and other health care personnel.Comment: Presented at 2016 Machine Learning and Healthcare Conference (MLHC\n  2016), Los Angeles, C",
      "word_count": 190,
      "core_id": 24821820,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321830"
    },
    {
      "title": "Towards a New Science of a Clinical Data Intelligence",
      "abstract": "In this paper we define Clinical Data Intelligence as the analysis of data\ngenerated in the clinical routine with the goal of improving patient care. We\ndefine a science of a Clinical Data Intelligence as a data analysis that\npermits the derivation of scientific, i.e., generalizable and reliable results.\nWe argue that a science of a Clinical Data Intelligence is sensible in the\ncontext of a Big Data analysis, i.e., with data from many patients and with\ncomplete patient information. We discuss that Clinical Data Intelligence\nrequires the joint efforts of knowledge engineering, information extraction\n(from textual and other unstructured data), and statistics and statistical\nmachine learning. We describe some of our main results as conjectures and\nrelate them to a recently funded research project involving two major German\nuniversity hospitals.Comment: NIPS 2013 Workshop: Machine Learning for Clinical Data Analysis and\n  Healthcare, 201",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "In this paper we define Clinical Data Intelligence as the analysis of data\ngenerated in the clinical routine with the goal of improving patient care. We\ndefine a science of a Clinical Data Intelligence as a data analysis that\npermits the derivation of scientific, i.e., generalizable and reliable results.\nWe argue that a science of a Clinical Data Intelligence is sensible in the\ncontext of a Big Data analysis, i.e., with data from many patients and with\ncomplete patient information. We discuss that Clinical Data Intelligence\nrequires the joint efforts of knowledge engineering, information extraction\n(from textual and other unstructured data), and statistics and statistical\nmachine learning. We describe some of our main results as conjectures and\nrelate them to a recently funded research project involving two major German\nuniversity hospitals.Comment: NIPS 2013 Workshop: Machine Learning for Clinical Data Analysis and\n  Healthcare, 201",
      "word_count": 143,
      "core_id": 17158793,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321850"
    },
    {
      "title": "Random projection to preserve patient privacy",
      "abstract": "With the availability of accessible and widely used cloud services, it is natural that large components of healthcare systems migrate to them; for example, patient databases can be stored and processed in the cloud. Such cloud services provide enhanced flexibility and additional gains, such as availability, ease of data share, and so on. This trend poses serious threats regarding the privacy of the patients and the trust that an individual must put into the healthcare system itself. Thus, there is a strong need of privacy preservation, achieved through a variety of different approaches. In this paper, we study the application of a random projection-based approach to patient data as a means to achieve two goals: (1) provably mask the identity of users under some adversarial-attack settings, (2) preserve enough information to allow for aggregate data analysis and application of machine-learning techniques. As far as we know, such approaches have not been applied and tested on medical data. We analyze the tradeoff between the loss of accuracy on the outcome of machine-learning algorithms and the resilience against an adversary. We show that random projections proved to be strong against known input/output attacks while offering high quality data, as long as the projected space is smaller than the original space, and as long as the amount of leaked data available to the adversary is limited",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "With the availability of accessible and widely used cloud services, it is natural that large components of healthcare systems migrate to them; for example, patient databases can be stored and processed in the cloud. Such cloud services provide enhanced flexibility and additional gains, such as availability, ease of data share, and so on. This trend poses serious threats regarding the privacy of the patients and the trust that an individual must put into the healthcare system itself. Thus, there is a strong need of privacy preservation, achieved through a variety of different approaches. In this paper, we study the application of a random projection-based approach to patient data as a means to achieve two goals: (1) provably mask the identity of users under some adversarial-attack settings, (2) preserve enough information to allow for aggregate data analysis and application of machine-learning techniques. As far as we know, such approaches have not been applied and tested on medical data. We analyze the tradeoff between the loss of accuracy on the outcome of machine-learning algorithms and the resilience against an adversary. We show that random projections proved to be strong against known input/output attacks while offering high quality data, as long as the projected space is smaller than the original space, and as long as the amount of leaked data available to the adversary is limited",
      "word_count": 223,
      "core_id": 82260619,
      "year": null,
      "processed_date": "2025-07-01T10:45:44.321875"
    },
    {
      "title": "Convolutional Sparse Kernel Network for Unsupervised Medical Image\n  Analysis",
      "abstract": "The availability of large-scale annotated image datasets and recent advances\nin supervised deep learning methods enable the end-to-end derivation of\nrepresentative image features that can impact a variety of image analysis\nproblems. Such supervised approaches, however, are difficult to implement in\nthe medical domain where large volumes of labelled data are difficult to obtain\ndue to the complexity of manual annotation and inter- and intra-observer\nvariability in label assignment. We propose a new convolutional sparse kernel\nnetwork (CSKN), which is a hierarchical unsupervised feature learning framework\nthat addresses the challenge of learning representative visual features in\nmedical image analysis domains where there is a lack of annotated training\ndata. Our framework has three contributions: (i) We extend kernel learning to\nidentify and represent invariant features across image sub-patches in an\nunsupervised manner. (ii) We initialise our kernel learning with a layer-wise\npre-training scheme that leverages the sparsity inherent in medical images to\nextract initial discriminative features. (iii) We adapt a multi-scale spatial\npyramid pooling (SPP) framework to capture subtle geometric differences between\nlearned visual features. We evaluated our framework in medical image retrieval\nand classification on three public datasets. Our results show that our CSKN had\nbetter accuracy when compared to other conventional unsupervised methods and\ncomparable accuracy to methods that used state-of-the-art supervised\nconvolutional neural networks (CNNs). Our findings indicate that our\nunsupervised CSKN provides an opportunity to leverage unannotated big data in\nmedical imaging repositories.Comment: Accepted by Medical Image Analysis (with a new title 'Convolutional\n  Sparse Kernel Network for Unsupervised Medical Image Analysis'). The\n  manuscript is available from following link\n  (https://doi.org/10.1016/j.media.2019.06.005",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "medicine",
      "content": "The availability of large-scale annotated image datasets and recent advances\nin supervised deep learning methods enable the end-to-end derivation of\nrepresentative image features that can impact a variety of image analysis\nproblems. Such supervised approaches, however, are difficult to implement in\nthe medical domain where large volumes of labelled data are difficult to obtain\ndue to the complexity of manual annotation and inter- and intra-observer\nvariability in label assignment. We propose a new convolutional sparse kernel\nnetwork (CSKN), which is a hierarchical unsupervised feature learning framework\nthat addresses the challenge of learning representative visual features in\nmedical image analysis domains where there is a lack of annotated training\ndata. Our framework has three contributions: (i) We extend kernel learning to\nidentify and represent invariant features across image sub-patches in an\nunsupervised manner. (ii) We initialise our kernel learning with a layer-wise\npre-training scheme that leverages the sparsity inherent in medical images to\nextract initial discriminative features. (iii) We adapt a multi-scale spatial\npyramid pooling (SPP) framework to capture subtle geometric differences between\nlearned visual features. We evaluated our framework in medical image retrieval\nand classification on three public datasets. Our results show that our CSKN had\nbetter accuracy when compared to other conventional unsupervised methods and\ncomparable accuracy to methods that used state-of-the-art supervised\nconvolutional neural networks (CNNs). Our findings indicate that our\nunsupervised CSKN provides an opportunity to leverage unannotated big data in\nmedical imaging repositories.Comment: Accepted by Medical Image Analysis (with a new title 'Convolutional\n  Sparse Kernel Network for Unsupervised Medical Image Analysis'). The\n  manuscript is available from following link\n  (https://doi.org/10.1016/j.media.2019.06.005",
      "word_count": 263,
      "core_id": 52222304,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.665882"
    },
    {
      "title": "MedGAN: Medical Image Translation using GANs",
      "abstract": "Image-to-image translation is considered a new frontier in the field of\nmedical image analysis, with numerous potential applications. However, a large\nportion of recent approaches offers individualized solutions based on\nspecialized task-specific architectures or require refinement through\nnon-end-to-end training. In this paper, we propose a new framework, named\nMedGAN, for medical image-to-image translation which operates on the image\nlevel in an end-to-end manner. MedGAN builds upon recent advances in the field\nof generative adversarial networks (GANs) by merging the adversarial framework\nwith a new combination of non-adversarial losses. We utilize a discriminator\nnetwork as a trainable feature extractor which penalizes the discrepancy\nbetween the translated medical images and the desired modalities. Moreover,\nstyle-transfer losses are utilized to match the textures and fine-structures of\nthe desired target images to the translated images. Additionally, we present a\nnew generator architecture, titled CasNet, which enhances the sharpness of the\ntranslated medical outputs through progressive refinement via encoder-decoder\npairs. Without any application-specific modifications, we apply MedGAN on three\ndifferent tasks: PET-CT translation, correction of MR motion artefacts and PET\nimage denoising. Perceptual analysis by radiologists and quantitative\nevaluations illustrate that the MedGAN outperforms other existing translation\napproaches.Comment: 16 pages, 8 figure",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "medicine",
      "content": "Image-to-image translation is considered a new frontier in the field of\nmedical image analysis, with numerous potential applications. However, a large\nportion of recent approaches offers individualized solutions based on\nspecialized task-specific architectures or require refinement through\nnon-end-to-end training. In this paper, we propose a new framework, named\nMedGAN, for medical image-to-image translation which operates on the image\nlevel in an end-to-end manner. MedGAN builds upon recent advances in the field\nof generative adversarial networks (GANs) by merging the adversarial framework\nwith a new combination of non-adversarial losses. We utilize a discriminator\nnetwork as a trainable feature extractor which penalizes the discrepancy\nbetween the translated medical images and the desired modalities. Moreover,\nstyle-transfer losses are utilized to match the textures and fine-structures of\nthe desired target images to the translated images. Additionally, we present a\nnew generator architecture, titled CasNet, which enhances the sharpness of the\ntranslated medical outputs through progressive refinement via encoder-decoder\npairs. Without any application-specific modifications, we apply MedGAN on three\ndifferent tasks: PET-CT translation, correction of MR motion artefacts and PET\nimage denoising. Perceptual analysis by radiologists and quantitative\nevaluations illustrate that the MedGAN outperforms other existing translation\napproaches.Comment: 16 pages, 8 figure",
      "word_count": 197,
      "core_id": 52213211,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.665908"
    },
    {
      "title": "NiftyNet: a deep-learning platform for medical imaging",
      "abstract": "Medical image analysis and computer-assisted intervention problems are\nincreasingly being addressed with deep-learning-based solutions. Established\ndeep-learning platforms are flexible but do not provide specific functionality\nfor medical image analysis and adapting them for this application requires\nsubstantial implementation effort. Thus, there has been substantial duplication\nof effort and incompatible infrastructure developed across many research\ngroups. This work presents the open-source NiftyNet platform for deep learning\nin medical imaging. The ambition of NiftyNet is to accelerate and simplify the\ndevelopment of these solutions, and to provide a common mechanism for\ndisseminating research outputs for the community to use, adapt and build upon.\n  NiftyNet provides a modular deep-learning pipeline for a range of medical\nimaging applications including segmentation, regression, image generation and\nrepresentation learning applications. Components of the NiftyNet pipeline\nincluding data loading, data augmentation, network architectures, loss\nfunctions and evaluation metrics are tailored to, and take advantage of, the\nidiosyncracies of medical image analysis and computer-assisted intervention.\nNiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D\nand 3D images and computational graphs by default.\n  We present 3 illustrative medical image analysis applications built using\nNiftyNet: (1) segmentation of multiple abdominal organs from computed\ntomography; (2) image regression to predict computed tomography attenuation\nmaps from brain magnetic resonance images; and (3) generation of simulated\nultrasound images for specified anatomical poses.\n  NiftyNet enables researchers to rapidly develop and distribute deep learning\nsolutions for segmentation, regression, image generation and representation\nlearning applications, or extend the platform to new applications.Comment: Wenqi Li and Eli Gibson contributed equally to this work. M. Jorge\n  Cardoso and Tom Vercauteren contributed equally to this work. 26 pages, 6\n  figures; Update includes additional applications, updated author list and\n  formatting for journal submissio",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "Medical image analysis and computer-assisted intervention problems are\nincreasingly being addressed with deep-learning-based solutions. Established\ndeep-learning platforms are flexible but do not provide specific functionality\nfor medical image analysis and adapting them for this application requires\nsubstantial implementation effort. Thus, there has been substantial duplication\nof effort and incompatible infrastructure developed across many research\ngroups. This work presents the open-source NiftyNet platform for deep learning\nin medical imaging. The ambition of NiftyNet is to accelerate and simplify the\ndevelopment of these solutions, and to provide a common mechanism for\ndisseminating research outputs for the community to use, adapt and build upon.\n  NiftyNet provides a modular deep-learning pipeline for a range of medical\nimaging applications including segmentation, regression, image generation and\nrepresentation learning applications. Components of the NiftyNet pipeline\nincluding data loading, data augmentation, network architectures, loss\nfunctions and evaluation metrics are tailored to, and take advantage of, the\nidiosyncracies of medical image analysis and computer-assisted intervention.\nNiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D\nand 3D images and computational graphs by default.\n  We present 3 illustrative medical image analysis applications built using\nNiftyNet: (1) segmentation of multiple abdominal organs from computed\ntomography; (2) image regression to predict computed tomography attenuation\nmaps from brain magnetic resonance images; and (3) generation of simulated\nultrasound images for specified anatomical poses.\n  NiftyNet enables researchers to rapidly develop and distribute deep learning\nsolutions for segmentation, regression, image generation and representation\nlearning applications, or extend the platform to new applications.Comment: Wenqi Li and Eli Gibson contributed equally to this work. M. Jorge\n  Cardoso and Tom Vercauteren contributed equally to this work. 26 pages, 6\n  figures; Update includes additional applications, updated author list and\n  formatting for journal submissio",
      "word_count": 284,
      "core_id": 44604214,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.665945"
    },
    {
      "title": "A Survey on Deep Learning in Medical Image Analysis",
      "abstract": "Deep learning algorithms, in particular convolutional networks, have rapidly\nbecome a methodology of choice for analyzing medical images. This paper reviews\nthe major deep learning concepts pertinent to medical image analysis and\nsummarizes over 300 contributions to the field, most of which appeared in the\nlast year. We survey the use of deep learning for image classification, object\ndetection, segmentation, registration, and other tasks and provide concise\noverviews of studies per application area. Open challenges and directions for\nfuture research are discussed.Comment: Revised survey includes expanded discussion section and reworked\n  introductory section on common deep architectures. Added missed papers from\n  before Feb 1st 201",
      "keywords": [],
      "paper_type": "review",
      "field": "medicine",
      "content": "Deep learning algorithms, in particular convolutional networks, have rapidly\nbecome a methodology of choice for analyzing medical images. This paper reviews\nthe major deep learning concepts pertinent to medical image analysis and\nsummarizes over 300 contributions to the field, most of which appeared in the\nlast year. We survey the use of deep learning for image classification, object\ndetection, segmentation, registration, and other tasks and provide concise\noverviews of studies per application area. Open challenges and directions for\nfuture research are discussed.Comment: Revised survey includes expanded discussion section and reworked\n  introductory section on common deep architectures. Added missed papers from\n  before Feb 1st 201",
      "word_count": 104,
      "core_id": 39078620,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.665954"
    },
    {
      "title": "Research\nPattern Classification using imaging techniques for Infarct and\nHemorrhage Identification in the Human Brain",
      "abstract": "Medical Image analysis and processing has great\nsignificance in the field of medicine, especially in Non-\ninvasive treatment and clinical study. Medical imaging\ntechniques and analysis tools enable the Doctors and\nRadiologists to arrive at a specific diagnosis. Medical Image\nProcessing has emerged as one of the most important tools\nto identify as well as diagnose various disorders. Imaging\nhelps the Doctors to visualize and analyze the image for\nunderstanding of abnormalities in internal structures. The\nmedical images data obtained from Bio-medical Devices\nwhich use imaging techniques like Computed Tomography\n(CT), Magnetic Resonance Imaging (MRI) and\nMammogram, which indicates the presence or absence of\nthe lesion along with the patient history, is an important\nfactor in the diagnosis. The algorithm proposes the use of\nDigital Image processing tools for the identification of\nHemorrhage and Infarct in the human brain, by using a\nsemi-automatic seeded region growing algorithm for the\nprocessing of the clinical images. The algorithm has been\nextended to the Real-Time Data of CT brain images and\nuses an intensity-based growing technique to identify the\ninfarct and hemorrhage affected area, of the brain. The\nobjective of this paper is to propose a seeded region\ngrowing algorithm to assist the Radiologists in identifying\nthe Hemorrhage and Infarct in the human brain and to arrive\nat a decision faster and accurate.¢Lp¤",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "Medical Image analysis and processing has great\nsignificance in the field of medicine, especially in Non-\ninvasive treatment and clinical study. Medical imaging\ntechniques and analysis tools enable the Doctors and\nRadiologists to arrive at a specific diagnosis. Medical Image\nProcessing has emerged as one of the most important tools\nto identify as well as diagnose various disorders. Imaging\nhelps the Doctors to visualize and analyze the image for\nunderstanding of abnormalities in internal structures. The\nmedical images data obtained from Bio-medical Devices\nwhich use imaging techniques like Computed Tomography\n(CT), Magnetic Resonance Imaging (MRI) and\nMammogram, which indicates the presence or absence of\nthe lesion along with the patient history, is an important\nfactor in the diagnosis. The algorithm proposes the use of\nDigital Image processing tools for the identification of\nHemorrhage and Infarct in the human brain, by using a\nsemi-automatic seeded region growing algorithm for the\nprocessing of the clinical images. The algorithm has been\nextended to the Real-Time Data of CT brain images and\nuses an intensity-based growing technique to identify the\ninfarct and hemorrhage affected area, of the brain. The\nobjective of this paper is to propose a seeded region\ngrowing algorithm to assist the Radiologists in identifying\nthe Hemorrhage and Infarct in the human brain and to arrive\nat a decision faster and accurate.¢Lp¤",
      "word_count": 220,
      "core_id": 259879,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666001"
    },
    {
      "title": "A Deep Learning Framework for Unsupervised Affine and Deformable Image\n  Registration",
      "abstract": "Image registration, the process of aligning two or more images, is the core\ntechnique of many (semi-)automatic medical image analysis tasks. Recent studies\nhave shown that deep learning methods, notably convolutional neural networks\n(ConvNets), can be used for image registration. Thus far training of ConvNets\nfor registration was supervised using predefined example registrations.\nHowever, obtaining example registrations is not trivial. To circumvent the need\nfor predefined examples, and thereby to increase convenience of training\nConvNets for image registration, we propose the Deep Learning Image\nRegistration (DLIR) framework for \\textit{unsupervised} affine and deformable\nimage registration. In the DLIR framework ConvNets are trained for image\nregistration by exploiting image similarity analogous to conventional\nintensity-based image registration. After a ConvNet has been trained with the\nDLIR framework, it can be used to register pairs of unseen images in one shot.\nWe propose flexible ConvNets designs for affine image registration and for\ndeformable image registration. By stacking multiple of these ConvNets into a\nlarger architecture, we are able to perform coarse-to-fine image registration.\nWe show for registration of cardiac cine MRI and registration of chest CT that\nperformance of the DLIR framework is comparable to conventional image\nregistration while being several orders of magnitude faster.Comment: Accepted: Medical Image Analysis - Elsevie",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "medicine",
      "content": "Image registration, the process of aligning two or more images, is the core\ntechnique of many (semi-)automatic medical image analysis tasks. Recent studies\nhave shown that deep learning methods, notably convolutional neural networks\n(ConvNets), can be used for image registration. Thus far training of ConvNets\nfor registration was supervised using predefined example registrations.\nHowever, obtaining example registrations is not trivial. To circumvent the need\nfor predefined examples, and thereby to increase convenience of training\nConvNets for image registration, we propose the Deep Learning Image\nRegistration (DLIR) framework for \\textit{unsupervised} affine and deformable\nimage registration. In the DLIR framework ConvNets are trained for image\nregistration by exploiting image similarity analogous to conventional\nintensity-based image registration. After a ConvNet has been trained with the\nDLIR framework, it can be used to register pairs of unseen images in one shot.\nWe propose flexible ConvNets designs for affine image registration and for\ndeformable image registration. By stacking multiple of these ConvNets into a\nlarger architecture, we are able to perform coarse-to-fine image registration.\nWe show for registration of cardiac cine MRI and registration of chest CT that\nperformance of the DLIR framework is comparable to conventional image\nregistration while being several orders of magnitude faster.Comment: Accepted: Medical Image Analysis - Elsevie",
      "word_count": 207,
      "core_id": 54152692,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666018"
    },
    {
      "title": "Journal Staff",
      "abstract": "This book constitutes the refereed proceedings of the 18th Scandinavian Conference on Image Analysis, SCIA 2013, held in Espoo, Finland, in June 2013. The 67 revised full papers presented were carefully reviewed and selected from 132 submissions. The papers are organized in topical sections on feature extraction and segmentation, pattern recognition and machine learning, medical and biomedical image analysis, faces and gestures, object and scene recognition, matching, registration, and alignment, 3D vision, color and multispectral image analysis, motion analysis, systems and applications, human-centered computing, and video and multimedia analysis",
      "keywords": [],
      "paper_type": "review",
      "field": "medicine",
      "content": "This book constitutes the refereed proceedings of the 18th Scandinavian Conference on Image Analysis, SCIA 2013, held in Espoo, Finland, in June 2013. The 67 revised full papers presented were carefully reviewed and selected from 132 submissions. The papers are organized in topical sections on feature extraction and segmentation, pattern recognition and machine learning, medical and biomedical image analysis, faces and gestures, object and scene recognition, matching, registration, and alignment, 3D vision, color and multispectral image analysis, motion analysis, systems and applications, human-centered computing, and video and multimedia analysis",
      "word_count": 89,
      "core_id": 21017704,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666024"
    },
    {
      "title": "Integrated Chest Image Analysis System \"BU-MIA\"",
      "abstract": "We introduce \"BU-MIA,\" a Medical Image Analysis system that integrates various advanced chest image analysis methods for detection, estimation, segmentation, and registration. BU-MIA evaluates repeated computed tomography (CT) scans of the same patient to facilitate identification and evaluation of pulmonary nodules for interval growth. It provides a user-friendly graphical user interface with a number of interaction tools for development, evaluation, and validation of chest image analysis methods. The structures that BU-MIA processes include the thorax, lungs, and trachea, pulmonary structures, such as lobes, fissures, nodules, and vessels, and bones, such as sternum, vertebrae, and ribs",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "We introduce \"BU-MIA,\" a Medical Image Analysis system that integrates various advanced chest image analysis methods for detection, estimation, segmentation, and registration. BU-MIA evaluates repeated computed tomography (CT) scans of the same patient to facilitate identification and evaluation of pulmonary nodules for interval growth. It provides a user-friendly graphical user interface with a number of interaction tools for development, evaluation, and validation of chest image analysis methods. The structures that BU-MIA processes include the thorax, lungs, and trachea, pulmonary structures, such as lobes, fissures, nodules, and vessels, and bones, such as sternum, vertebrae, and ribs",
      "word_count": 95,
      "core_id": 46207348,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666046"
    },
    {
      "title": "A Latent Source Model for Patch-Based Image Segmentation",
      "abstract": "Despite the popularity and empirical success of patch-based nearest-neighbor\nand weighted majority voting approaches to medical image segmentation, there\nhas been no theoretical development on when, why, and how well these\nnonparametric methods work. We bridge this gap by providing a theoretical\nperformance guarantee for nearest-neighbor and weighted majority voting\nsegmentation under a new probabilistic model for patch-based image\nsegmentation. Our analysis relies on a new local property for how similar\nnearby patches are, and fuses existing lines of work on modeling natural\nimagery patches and theory for nonparametric classification. We use the model\nto derive a new patch-based segmentation algorithm that iterates between\ninferring local label patches and merging these local segmentations to produce\na globally consistent image segmentation. Many existing patch-based algorithms\narise as special cases of the new algorithm.Comment: International Conference on Medical Image Computing and Computer\n  Assisted Interventions 201",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "medicine",
      "content": "Despite the popularity and empirical success of patch-based nearest-neighbor\nand weighted majority voting approaches to medical image segmentation, there\nhas been no theoretical development on when, why, and how well these\nnonparametric methods work. We bridge this gap by providing a theoretical\nperformance guarantee for nearest-neighbor and weighted majority voting\nsegmentation under a new probabilistic model for patch-based image\nsegmentation. Our analysis relies on a new local property for how similar\nnearby patches are, and fuses existing lines of work on modeling natural\nimagery patches and theory for nonparametric classification. We use the model\nto derive a new patch-based segmentation algorithm that iterates between\ninferring local label patches and merging these local segmentations to produce\na globally consistent image segmentation. Many existing patch-based algorithms\narise as special cases of the new algorithm.Comment: International Conference on Medical Image Computing and Computer\n  Assisted Interventions 201",
      "word_count": 143,
      "core_id": 24734400,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666058"
    },
    {
      "title": "Medical imaging analysis with artificial neural networks",
      "abstract": "Given that neural networks have been widely reported in the research community of medical imaging, we provide a focused literature survey on recent neural network developments in computer-aided diagnosis, medical image segmentation and edge detection towards visual content analysis, and medical image registration for its pre-processing and post-processing, with the aims of increasing awareness of how neural networks can be applied to these areas and to provide a foundation for further research and practical development. Representative techniques and algorithms are explained in detail to provide inspiring examples illustrating: (i) how a known neural network with fixed structure and training procedure could be applied to resolve a medical imaging problem; (ii) how medical images could be analysed, processed, and characterised by neural networks; and (iii) how neural networks could be expanded further to resolve problems relevant to medical imaging. In the concluding section, a highlight of comparisons among many neural network applications is included to provide a global view on computational intelligence with neural networks in medical imaging",
      "keywords": [],
      "paper_type": "review",
      "field": "medicine",
      "content": "Given that neural networks have been widely reported in the research community of medical imaging, we provide a focused literature survey on recent neural network developments in computer-aided diagnosis, medical image segmentation and edge detection towards visual content analysis, and medical image registration for its pre-processing and post-processing, with the aims of increasing awareness of how neural networks can be applied to these areas and to provide a foundation for further research and practical development. Representative techniques and algorithms are explained in detail to provide inspiring examples illustrating: (i) how a known neural network with fixed structure and training procedure could be applied to resolve a medical imaging problem; (ii) how medical images could be analysed, processed, and characterised by neural networks; and (iii) how neural networks could be expanded further to resolve problems relevant to medical imaging. In the concluding section, a highlight of comparisons among many neural network applications is included to provide a global view on computational intelligence with neural networks in medical imaging",
      "word_count": 167,
      "core_id": 4201783,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666068"
    },
    {
      "title": "Self-paced Convolutional Neural Network for Computer Aided Detection in\n  Medical Imaging Analysis",
      "abstract": "Tissue characterization has long been an important component of Computer\nAided Diagnosis (CAD) systems for automatic lesion detection and further\nclinical planning. Motivated by the superior performance of deep learning\nmethods on various computer vision problems, there has been increasing work\napplying deep learning to medical image analysis. However, the development of a\nrobust and reliable deep learning model for computer-aided diagnosis is still\nhighly challenging due to the combination of the high heterogeneity in the\nmedical images and the relative lack of training samples. Specifically,\nannotation and labeling of the medical images is much more expensive and\ntime-consuming than other applications and often involves manual labor from\nmultiple domain experts. In this work, we propose a multi-stage, self-paced\nlearning framework utilizing a convolutional neural network (CNN) to classify\nComputed Tomography (CT) image patches. The key contribution of this approach\nis that we augment the size of training samples by refining the unlabeled\ninstances with a self-paced learning CNN. By implementing the framework on high\nperformance computing servers including the NVIDIA DGX1 machine, we obtained\nthe experimental result, showing that the self-pace boosted network\nconsistently outperformed the original network even with very scarce manual\nlabels. The performance gain indicates that applications with limited training\nsamples such as medical image analysis can benefit from using the proposed\nframework.Comment: accepted by 8th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2017",
      "keywords": [],
      "paper_type": "theoretical",
      "field": "medicine",
      "content": "Tissue characterization has long been an important component of Computer\nAided Diagnosis (CAD) systems for automatic lesion detection and further\nclinical planning. Motivated by the superior performance of deep learning\nmethods on various computer vision problems, there has been increasing work\napplying deep learning to medical image analysis. However, the development of a\nrobust and reliable deep learning model for computer-aided diagnosis is still\nhighly challenging due to the combination of the high heterogeneity in the\nmedical images and the relative lack of training samples. Specifically,\nannotation and labeling of the medical images is much more expensive and\ntime-consuming than other applications and often involves manual labor from\nmultiple domain experts. In this work, we propose a multi-stage, self-paced\nlearning framework utilizing a convolutional neural network (CNN) to classify\nComputed Tomography (CT) image patches. The key contribution of this approach\nis that we augment the size of training samples by refining the unlabeled\ninstances with a self-paced learning CNN. By implementing the framework on high\nperformance computing servers including the NVIDIA DGX1 machine, we obtained\nthe experimental result, showing that the self-pace boosted network\nconsistently outperformed the original network even with very scarce manual\nlabels. The performance gain indicates that applications with limited training\nsamples such as medical image analysis can benefit from using the proposed\nframework.Comment: accepted by 8th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2017",
      "word_count": 230,
      "core_id": 43123569,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666086"
    },
    {
      "title": "Fast Predictive Simple Geodesic Regression",
      "abstract": "Deformable image registration and regression are important tasks in medical\nimage analysis. However, they are computationally expensive, especially when\nanalyzing large-scale datasets that contain thousands of images. Hence, cluster\ncomputing is typically used, making the approaches dependent on such\ncomputational infrastructure. Even larger computational resources are required\nas study sizes increase. This limits the use of deformable image registration\nand regression for clinical applications and as component algorithms for other\nimage analysis approaches. We therefore propose using a fast predictive\napproach to perform image registrations. In particular, we employ these fast\nregistration predictions to approximate a simplified geodesic regression model\nto capture longitudinal brain changes. The resulting method is orders of\nmagnitude faster than the standard optimization-based regression model and\nhence facilitates large-scale analysis on a single graphics processing unit\n(GPU). We evaluate our results on 3D brain magnetic resonance images (MRI) from\nthe ADNI datasets.Comment: 19 pages, 10 figures, 13 table",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "Deformable image registration and regression are important tasks in medical\nimage analysis. However, they are computationally expensive, especially when\nanalyzing large-scale datasets that contain thousands of images. Hence, cluster\ncomputing is typically used, making the approaches dependent on such\ncomputational infrastructure. Even larger computational resources are required\nas study sizes increase. This limits the use of deformable image registration\nand regression for clinical applications and as component algorithms for other\nimage analysis approaches. We therefore propose using a fast predictive\napproach to perform image registrations. In particular, we employ these fast\nregistration predictions to approximate a simplified geodesic regression model\nto capture longitudinal brain changes. The resulting method is orders of\nmagnitude faster than the standard optimization-based regression model and\nhence facilitates large-scale analysis on a single graphics processing unit\n(GPU). We evaluate our results on 3D brain magnetic resonance images (MRI) from\nthe ADNI datasets.Comment: 19 pages, 10 figures, 13 table",
      "word_count": 152,
      "core_id": 45158955,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666107"
    },
    {
      "title": "Multiresolution analysis using wavelet, ridgelet, and curvelet transforms for medical image segmentation",
      "abstract": "Copyright @ 2011 Shadi AlZubi et al. This article has been made available through the Brunel Open Access Publishing Fund.The experimental study presented in this paper is aimed at the development of an automatic image segmentation system for classifying region of interest (ROI) in medical images which are obtained from different medical scanners such as PET, CT, or MRI. Multiresolution analysis (MRA) using wavelet, ridgelet, and curvelet transforms has been used in the proposed segmentation system. It is particularly a challenging task to classify cancers in human organs in scanners output using shape or gray-level information; organs shape changes throw different slices in medical stack and the gray-level intensity overlap in soft tissues. Curvelet transform is a new extension of wavelet and ridgelet transforms which aims to deal with interesting phenomena occurring along curves. Curvelet transforms has been tested on medical data sets, and results are compared with those obtained from the other transforms. Tests indicate that using curvelet significantly improves the classification of abnormal tissues in the scans and reduce the surrounding noise",
      "keywords": [],
      "paper_type": "analytical",
      "field": "medicine",
      "content": "Copyright @ 2011 Shadi AlZubi et al. This article has been made available through the Brunel Open Access Publishing Fund.The experimental study presented in this paper is aimed at the development of an automatic image segmentation system for classifying region of interest (ROI) in medical images which are obtained from different medical scanners such as PET, CT, or MRI. Multiresolution analysis (MRA) using wavelet, ridgelet, and curvelet transforms has been used in the proposed segmentation system. It is particularly a challenging task to classify cancers in human organs in scanners output using shape or gray-level information; organs shape changes throw different slices in medical stack and the gray-level intensity overlap in soft tissues. Curvelet transform is a new extension of wavelet and ridgelet transforms which aims to deal with interesting phenomena occurring along curves. Curvelet transforms has been tested on medical data sets, and results are compared with those obtained from the other transforms. Tests indicate that using curvelet significantly improves the classification of abnormal tissues in the scans and reduce the surrounding noise",
      "word_count": 174,
      "core_id": 348957,
      "year": null,
      "processed_date": "2025-07-01T10:45:46.666128"
    }
  ],
  "statistics": {
    "total_papers": 93,
    "paper_types": {
      "technical": 15,
      "analytical": 18,
      "methodological": 18,
      "empirical": 6,
      "comparative": 5,
      "theoretical": 17,
      "review": 14
    },
    "fields": {
      "computer_science": 58,
      "medicine": 25,
      "social_sciences": 7,
      "business": 3
    },
    "collection_date": "2025-07-01T10:46:00.301423"
  }
}